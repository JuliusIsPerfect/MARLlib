{"episode_reward_max": -2000.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2240.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -500.0, "policy_1": -500.0, "policy_2": -500.0, "policy_3": -500.0}, "policy_reward_mean": {"policy_0": -560.0, "policy_1": -560.0, "policy_2": -560.0, "policy_3": -560.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0], "policy_policy_1_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0], "policy_policy_2_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0], "policy_policy_3_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6867598595663784, "mean_inference_ms": 5.419822700214019, "mean_action_processing_ms": 0.14085179835246517, "mean_env_wait_ms": 9.574084667901769, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 20000, "timesteps_this_iter": 0, "agent_timesteps_total": 80000, "timers": {"sample_time_ms": 317293.533, "sample_throughput": 63.033, "load_time_ms": 1258.977, "load_throughput": 15885.912, "learn_time_ms": 176699.488, "learn_throughput": 113.187, "update_time_ms": 11.087}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 571.5631103515625, "cur_kl_coeff": 0.2, "cur_lr": 0.0005, "total_loss": 2744.442236328125, "policy_loss": 9.453010379023575e-07, "vf_loss": 2744.442236328125, "vf_explained_var": 1.0132789611816406e-05, "kl": 8.671107443181114e-10, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 444.23748779296875, "cur_kl_coeff": 0.2, "cur_lr": 0.0005, "total_loss": 2700.006689453125, "policy_loss": 9.77649688493898e-07, "vf_loss": 2700.006689453125, "vf_explained_var": -2.7418136596679688e-06, "kl": 2.5765412997591496e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 623.90283203125, "cur_kl_coeff": 0.2, "cur_lr": 0.0005, "total_loss": 2696.492626953125, "policy_loss": 9.402179827588953e-07, "vf_loss": 2696.492626953125, "vf_explained_var": 2.4378299713134766e-05, "kl": 9.425200175883219e-10, "entropy": 3.2188708782196045, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 550.3615112304688, "cur_kl_coeff": 0.2, "cur_lr": 0.0005, "total_loss": 2694.815673828125, "policy_loss": 8.981514208805663e-07, "vf_loss": 2694.815673828125, "vf_explained_var": -2.193450927734375e-05, "kl": 2.3759115119759143e-09, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 20000, "num_agent_steps_sampled": 80000, "num_steps_trained": 20000, "num_agent_steps_trained": 80000}, "done": false, "episodes_total": 10, "training_iteration": 1, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_11-06-57", "timestamp": 1704856017, "time_this_iter_s": 495.178031206131, "time_total_s": 495.178031206131, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170FC8ED310>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 495.178031206131, "timesteps_since_restore": 0, "iterations_since_restore": 1, "perf": {"cpu_util_percent": 21.047360912981457, "ram_util_percent": 50.45920114122683}}
{"episode_reward_max": -2000.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2240.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -500.0, "policy_1": -500.0, "policy_2": -500.0, "policy_3": -500.0}, "policy_reward_mean": {"policy_0": -560.0, "policy_1": -560.0, "policy_2": -560.0, "policy_3": -560.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0], "policy_policy_1_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0], "policy_policy_2_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0], "policy_policy_3_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6855981239869187, "mean_inference_ms": 5.404720495743693, "mean_action_processing_ms": 0.13755650018805513, "mean_env_wait_ms": 9.554427576156797, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 40000, "timesteps_this_iter": 0, "agent_timesteps_total": 160000, "timers": {"sample_time_ms": 404655.994, "sample_throughput": 49.425, "load_time_ms": 1252.16, "load_throughput": 15972.397, "learn_time_ms": 177381.938, "learn_throughput": 112.751, "update_time_ms": 11.526}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 547.7550048828125, "cur_kl_coeff": 0.1, "cur_lr": 0.0004950000001, "total_loss": 2692.701416015625, "policy_loss": 5.605792994356485e-07, "vf_loss": 2692.701416015625, "vf_explained_var": -5.364418029785156e-06, "kl": 1.775558342020922e-09, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 376.0611877441406, "cur_kl_coeff": 0.1, "cur_lr": 0.0004950000001, "total_loss": 2663.249951171875, "policy_loss": 5.174446120026488e-07, "vf_loss": 2663.249951171875, "vf_explained_var": 1.2993812561035156e-05, "kl": 6.311442202200368e-10, "entropy": 3.2188714027404783, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 336.27777099609375, "cur_kl_coeff": 0.1, "cur_lr": 0.0004950000001, "total_loss": 2658.1328125, "policy_loss": 4.619407742545434e-07, "vf_loss": 2658.1328125, "vf_explained_var": 7.867813110351562e-06, "kl": 1.6054163110013064e-09, "entropy": 3.218870449066162, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 366.70416259765625, "cur_kl_coeff": 0.1, "cur_lr": 0.0004950000001, "total_loss": 2659.72060546875, "policy_loss": 5.135631535146957e-07, "vf_loss": 2659.72060546875, "vf_explained_var": -2.2411346435546875e-05, "kl": -1.495772758777769e-09, "entropy": 3.2188718795776365, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 40000, "num_agent_steps_sampled": 160000, "num_steps_trained": 40000, "num_agent_steps_trained": 160000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 20, "training_iteration": 2, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_11-15-10", "timestamp": 1704856510, "time_this_iter_s": 493.3431191444397, "time_total_s": 988.5211503505707, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170443ACF70>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 988.5211503505707, "timesteps_since_restore": 0, "iterations_since_restore": 2, "perf": {"cpu_util_percent": 20.602722063037252, "ram_util_percent": 57.95429799426934}}
{"episode_reward_max": -2000.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2266.6666666666665, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -500.0, "policy_1": -500.0, "policy_2": -500.0, "policy_3": -500.0}, "policy_reward_mean": {"policy_0": -566.6666666666666, "policy_1": -566.6666666666666, "policy_2": -566.6666666666666, "policy_3": -566.6666666666666}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_1_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_2_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_3_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6884972153421693, "mean_inference_ms": 5.42034597350063, "mean_action_processing_ms": 0.1365076140828051, "mean_env_wait_ms": 9.57858544971674, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 60000, "timesteps_this_iter": 0, "agent_timesteps_total": 240000, "timers": {"sample_time_ms": 438087.451, "sample_throughput": 45.653, "load_time_ms": 1250.146, "load_throughput": 15998.137, "learn_time_ms": 176747.557, "learn_throughput": 113.156, "update_time_ms": 12.046}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 316.42498779296875, "cur_kl_coeff": 0.05, "cur_lr": 0.0004900000002, "total_loss": 2961.418701171875, "policy_loss": 3.957653033737074e-07, "vf_loss": 2961.418701171875, "vf_explained_var": 4.351139068603516e-06, "kl": 5.958874227385281e-11, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 335.0171203613281, "cur_kl_coeff": 0.05, "cur_lr": 0.0004900000002, "total_loss": 2938.80654296875, "policy_loss": 3.409099550988337e-07, "vf_loss": 2938.80654296875, "vf_explained_var": 2.4557113647460938e-05, "kl": 6.931784590769041e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 300.84906005859375, "cur_kl_coeff": 0.05, "cur_lr": 0.0004900000002, "total_loss": 2936.716357421875, "policy_loss": 5.610751994344909e-07, "vf_loss": 2936.716357421875, "vf_explained_var": 3.838539123535156e-05, "kl": 6.991144326645715e-10, "entropy": 3.218870449066162, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 313.0498046875, "cur_kl_coeff": 0.05, "cur_lr": 0.0004900000002, "total_loss": 2940.51279296875, "policy_loss": 3.625774306570406e-07, "vf_loss": 2940.51279296875, "vf_explained_var": -1.728534698486328e-05, "kl": 7.001085600144563e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 60000, "num_agent_steps_sampled": 240000, "num_steps_trained": 60000, "num_agent_steps_trained": 240000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 30, "training_iteration": 3, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_11-23-32", "timestamp": 1704857012, "time_this_iter_s": 502.3257176876068, "time_total_s": 1490.8468680381775, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170F9AB5F70>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 1490.8468680381775, "timesteps_since_restore": 0, "iterations_since_restore": 3, "perf": {"cpu_util_percent": 21.899154929577467, "ram_util_percent": 59.48352112676056}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2220.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -555.0, "policy_1": -555.0, "policy_2": -555.0, "policy_3": -555.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -1600.0, -1600.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0], "policy_policy_1_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0], "policy_policy_2_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0], "policy_policy_3_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6922311718908144, "mean_inference_ms": 5.449172193055235, "mean_action_processing_ms": 0.13628638817655658, "mean_env_wait_ms": 9.619190166985927, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 80000, "timesteps_this_iter": 0, "agent_timesteps_total": 320000, "timers": {"sample_time_ms": 456666.028, "sample_throughput": 43.796, "load_time_ms": 1253.017, "load_throughput": 15961.474, "learn_time_ms": 177503.767, "learn_throughput": 112.674, "update_time_ms": 12.02}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 321.60443115234375, "cur_kl_coeff": 0.025, "cur_lr": 0.00048500000029999996, "total_loss": 2523.233935546875, "policy_loss": 2.9227256899133635e-07, "vf_loss": 2523.233935546875, "vf_explained_var": 2.1457672119140625e-06, "kl": 6.871375191064288e-10, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 343.2181396484375, "cur_kl_coeff": 0.025, "cur_lr": 0.00048500000029999996, "total_loss": 2503.521875, "policy_loss": 3.4562110267089e-07, "vf_loss": 2503.521875, "vf_explained_var": 1.138448715209961e-05, "kl": 1.7872031077281746e-10, "entropy": 3.218870162963867, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 387.0935974121094, "cur_kl_coeff": 0.025, "cur_lr": 0.00048500000029999996, "total_loss": 2507.71123046875, "policy_loss": 4.2697906246225157e-07, "vf_loss": 2507.71123046875, "vf_explained_var": 2.7239322662353516e-05, "kl": -2.4361198197597745e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 299.48065185546875, "cur_kl_coeff": 0.025, "cur_lr": 0.00048500000029999996, "total_loss": 2509.846875, "policy_loss": 2.5086402839491483e-07, "vf_loss": 2509.846875, "vf_explained_var": -1.728534698486328e-05, "kl": -8.536982454288023e-10, "entropy": 3.218871593475342, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 80000, "num_agent_steps_sampled": 320000, "num_steps_trained": 80000, "num_agent_steps_trained": 320000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 40, "training_iteration": 4, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_11-32-09", "timestamp": 1704857529, "time_this_iter_s": 516.644791841507, "time_total_s": 2007.4916598796844, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170441CF040>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 2007.4916598796844, "timesteps_since_restore": 0, "iterations_since_restore": 4, "perf": {"cpu_util_percent": 24.325890410958905, "ram_util_percent": 62.381095890410954}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2208.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -552.0, "policy_1": -552.0, "policy_2": -552.0, "policy_3": -552.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -1600.0, -1600.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -1600.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0], "policy_policy_1_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0], "policy_policy_2_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0], "policy_policy_3_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6947410236411825, "mean_inference_ms": 5.468689331826852, "mean_action_processing_ms": 0.13616196070322215, "mean_env_wait_ms": 9.651198429865923, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 100000, "timesteps_this_iter": 0, "agent_timesteps_total": 400000, "timers": {"sample_time_ms": 467178.349, "sample_throughput": 42.81, "load_time_ms": 1252.952, "load_throughput": 15962.303, "learn_time_ms": 178234.259, "learn_throughput": 112.212, "update_time_ms": 11.815}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 350.25738525390625, "cur_kl_coeff": 0.0125, "cur_lr": 0.00048000000040000003, "total_loss": 2632.481982421875, "policy_loss": 2.906703970850799e-07, "vf_loss": 2632.481982421875, "vf_explained_var": 1.2636184692382812e-05, "kl": 9.637351763847057e-10, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 381.7786865234375, "cur_kl_coeff": 0.0125, "cur_lr": 0.00048000000040000003, "total_loss": 2615.21806640625, "policy_loss": 4.6710014229844886e-07, "vf_loss": 2615.21806640625, "vf_explained_var": 1.4483928680419922e-05, "kl": 1.0417890022873522e-09, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 310.42193603515625, "cur_kl_coeff": 0.0125, "cur_lr": 0.00048000000040000003, "total_loss": 2623.18046875, "policy_loss": 3.643322034174723e-07, "vf_loss": 2623.18046875, "vf_explained_var": 2.7477741241455078e-05, "kl": 1.7951619579736899e-09, "entropy": 3.2188699722290037, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 354.9029235839844, "cur_kl_coeff": 0.0125, "cur_lr": 0.00048000000040000003, "total_loss": 2623.312841796875, "policy_loss": 2.973270409545314e-07, "vf_loss": 2623.312841796875, "vf_explained_var": -3.516674041748047e-05, "kl": -1.0865955486077938e-09, "entropy": 3.2188720226287844, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 100000, "num_agent_steps_sampled": 400000, "num_steps_trained": 100000, "num_agent_steps_trained": 400000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 50, "training_iteration": 5, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_11-40-40", "timestamp": 1704858040, "time_this_iter_s": 510.552227973938, "time_total_s": 2518.0438878536224, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A5820>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 2518.0438878536224, "timesteps_since_restore": 0, "iterations_since_restore": 5, "perf": {"cpu_util_percent": 22.73509015256588, "ram_util_percent": 62.49375866851595}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2206.6666666666665, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -551.6666666666666, "policy_1": -551.6666666666666, "policy_2": -551.6666666666666, "policy_3": -551.6666666666666}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -1600.0, -1600.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0], "policy_policy_1_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0], "policy_policy_2_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0], "policy_policy_3_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6953020365966612, "mean_inference_ms": 5.472361665891396, "mean_action_processing_ms": 0.13598015286058462, "mean_env_wait_ms": 9.660925335904768, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 120000, "timesteps_this_iter": 0, "agent_timesteps_total": 480000, "timers": {"sample_time_ms": 471074.696, "sample_throughput": 42.456, "load_time_ms": 1249.234, "load_throughput": 16009.812, "learn_time_ms": 177458.996, "learn_throughput": 112.702, "update_time_ms": 11.687}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 359.60272216796875, "cur_kl_coeff": 0.00625, "cur_lr": 0.00047500000050000005, "total_loss": 2573.44453125, "policy_loss": 3.546237953733211e-07, "vf_loss": 2573.44453125, "vf_explained_var": 1.919269561767578e-05, "kl": 2.401956943065553e-09, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 423.23052978515625, "cur_kl_coeff": 0.00625, "cur_lr": 0.00047500000050000005, "total_loss": 2560.6689453125, "policy_loss": 2.0065307482219907e-07, "vf_loss": 2560.6689453125, "vf_explained_var": 1.996755599975586e-05, "kl": 4.705446310371286e-10, "entropy": 3.2188701152801515, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 383.8805236816406, "cur_kl_coeff": 0.00625, "cur_lr": 0.00047500000050000005, "total_loss": 2566.448291015625, "policy_loss": 3.565978923703028e-07, "vf_loss": 2566.448291015625, "vf_explained_var": 1.5854835510253906e-05, "kl": 6.663649965155116e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 346.262939453125, "cur_kl_coeff": 0.00625, "cur_lr": 0.00047500000050000005, "total_loss": 2569.7716796875, "policy_loss": 3.3916473376471854e-07, "vf_loss": 2569.7716796875, "vf_explained_var": -1.2516975402832031e-05, "kl": -7.116042824439894e-11, "entropy": 3.2188719272613526, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 120000, "num_agent_steps_sampled": 480000, "num_steps_trained": 120000, "num_agent_steps_trained": 480000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 60, "training_iteration": 6, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_11-48-43", "timestamp": 1704858523, "time_this_iter_s": 482.9123270511627, "time_total_s": 3000.956214904785, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170442301F0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 3000.956214904785, "timesteps_since_restore": 0, "iterations_since_restore": 6, "perf": {"cpu_util_percent": 23.853801169590643, "ram_util_percent": 60.95014619883041}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2200.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -550.0, "policy_1": -550.0, "policy_2": -550.0, "policy_3": -550.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -1600.0, -1600.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0], "policy_policy_1_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0], "policy_policy_2_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0], "policy_policy_3_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6940847203704982, "mean_inference_ms": 5.463904338784008, "mean_action_processing_ms": 0.13568233217889286, "mean_env_wait_ms": 9.655079486917456, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 140000, "timesteps_this_iter": 0, "agent_timesteps_total": 560000, "timers": {"sample_time_ms": 471073.866, "sample_throughput": 42.456, "load_time_ms": 1250.275, "load_throughput": 15996.482, "learn_time_ms": 176218.655, "learn_throughput": 113.495, "update_time_ms": 11.728}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 389.6362609863281, "cur_kl_coeff": 0.003125, "cur_lr": 0.0004700000006, "total_loss": 2470.8125, "policy_loss": 2.0722389475480442e-07, "vf_loss": 2470.8125, "vf_explained_var": -8.106231689453125e-06, "kl": 1.6073329667243287e-09, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 405.2106018066406, "cur_kl_coeff": 0.003125, "cur_lr": 0.0004700000006, "total_loss": 2453.662451171875, "policy_loss": 3.5970687299879956e-07, "vf_loss": 2453.662451171875, "vf_explained_var": 1.4603137969970703e-05, "kl": 7.948856128869863e-10, "entropy": 3.2188701152801515, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 400.7884216308594, "cur_kl_coeff": 0.003125, "cur_lr": 0.0004700000006, "total_loss": 2463.8220703125, "policy_loss": 3.6236763332198054e-07, "vf_loss": 2463.8220703125, "vf_explained_var": 2.3186206817626953e-05, "kl": 1.1054951398747903e-09, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 407.98724365234375, "cur_kl_coeff": 0.003125, "cur_lr": 0.0004700000006, "total_loss": 2465.7740234375, "policy_loss": 1.2113571479233087e-07, "vf_loss": 2465.7740234375, "vf_explained_var": -3.159046173095703e-05, "kl": -6.974926081149136e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 140000, "num_agent_steps_sampled": 560000, "num_steps_trained": 140000, "num_agent_steps_trained": 560000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 70, "training_iteration": 7, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_11-56-29", "timestamp": 1704858989, "time_this_iter_s": 466.2414345741272, "time_total_s": 3467.1976494789124, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x0000017044293310>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 3467.1976494789124, "timesteps_since_restore": 0, "iterations_since_restore": 7, "perf": {"cpu_util_percent": 17.715454545454545, "ram_util_percent": 60.556969696969695}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2205.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -551.25, "policy_1": -551.25, "policy_2": -551.25, "policy_3": -551.25}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -1600.0, -1600.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0], "policy_policy_1_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0], "policy_policy_2_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0], "policy_policy_3_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6921692882359856, "mean_inference_ms": 5.450131567100868, "mean_action_processing_ms": 0.13534833090975962, "mean_env_wait_ms": 9.642856721498218, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 160000, "timesteps_this_iter": 0, "agent_timesteps_total": 640000, "timers": {"sample_time_ms": 470545.876, "sample_throughput": 42.504, "load_time_ms": 1251.878, "load_throughput": 15975.995, "learn_time_ms": 175241.668, "learn_throughput": 114.128, "update_time_ms": 11.643}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 525.0746459960938, "cur_kl_coeff": 0.0015625, "cur_lr": 0.0004650000007, "total_loss": 2548.091845703125, "policy_loss": 2.0885467515441293e-07, "vf_loss": 2548.091845703125, "vf_explained_var": -1.1682510375976562e-05, "kl": 9.172699266812856e-10, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 474.9041442871094, "cur_kl_coeff": 0.0015625, "cur_lr": 0.0004650000007, "total_loss": 2542.10390625, "policy_loss": 3.5655021481950653e-07, "vf_loss": 2542.10390625, "vf_explained_var": 3.2186508178710938e-06, "kl": 5.320843515788099e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 613.7033081054688, "cur_kl_coeff": 0.0015625, "cur_lr": 0.0004650000007, "total_loss": 2548.9990234375, "policy_loss": 4.5889854281000455e-07, "vf_loss": 2548.9990234375, "vf_explained_var": 2.467632293701172e-05, "kl": -1.088137954741164e-10, "entropy": 3.2188701152801515, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 497.53564453125, "cur_kl_coeff": 0.0015625, "cur_lr": 0.0004650000007, "total_loss": 2548.8314453125, "policy_loss": 3.0477523873528865e-07, "vf_loss": 2548.8314453125, "vf_explained_var": -2.2292137145996094e-05, "kl": 2.850512577368125e-09, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 160000, "num_agent_steps_sampled": 640000, "num_steps_trained": 160000, "num_agent_steps_trained": 640000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 80, "training_iteration": 8, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_12-04-15", "timestamp": 1704859455, "time_this_iter_s": 466.42946124076843, "time_total_s": 3933.627110719681, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710F113310>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 3933.627110719681, "timesteps_since_restore": 0, "iterations_since_restore": 8, "perf": {"cpu_util_percent": 17.729954614220876, "ram_util_percent": 61.50907715582451}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2204.4444444444443, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -551.1111111111111, "policy_1": -551.1111111111111, "policy_2": -551.1111111111111, "policy_3": -551.1111111111111}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -1600.0, -1600.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0], "policy_policy_1_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0], "policy_policy_2_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0], "policy_policy_3_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.689969066118854, "mean_inference_ms": 5.434419320584804, "mean_action_processing_ms": 0.13503792769729178, "mean_env_wait_ms": 9.62713419065638, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 180000, "timesteps_this_iter": 0, "agent_timesteps_total": 720000, "timers": {"sample_time_ms": 469979.256, "sample_throughput": 42.555, "load_time_ms": 1245.911, "load_throughput": 16052.506, "learn_time_ms": 174467.268, "learn_throughput": 114.635, "update_time_ms": 11.571}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 557.0497436523438, "cur_kl_coeff": 0.00078125, "cur_lr": 0.0004600000008, "total_loss": 2349.145556640625, "policy_loss": 2.6247977593651937e-07, "vf_loss": 2349.145556640625, "vf_explained_var": -1.0371208190917969e-05, "kl": -1.5866751523763157e-10, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 719.6956787109375, "cur_kl_coeff": 0.00078125, "cur_lr": 0.0004600000008, "total_loss": 2347.615283203125, "policy_loss": 5.477237781548183e-07, "vf_loss": 2347.615283203125, "vf_explained_var": 7.867813110351562e-06, "kl": 1.7684044617549688e-09, "entropy": 3.218870496749878, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 560.9693603515625, "cur_kl_coeff": 0.00078125, "cur_lr": 0.0004600000008, "total_loss": 2344.36923828125, "policy_loss": 3.4502983528028606e-07, "vf_loss": 2344.36923828125, "vf_explained_var": 2.2232532501220703e-05, "kl": 1.3689253314241867e-09, "entropy": 3.21887001991272, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 554.4022827148438, "cur_kl_coeff": 0.00078125, "cur_lr": 0.0004600000008, "total_loss": 2341.066845703125, "policy_loss": 2.2921562816691222e-07, "vf_loss": 2341.066845703125, "vf_explained_var": -1.5139579772949219e-05, "kl": 1.2891112656510106e-10, "entropy": 3.2188718795776365, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 180000, "num_agent_steps_sampled": 720000, "num_steps_trained": 180000, "num_agent_steps_trained": 720000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 90, "training_iteration": 9, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_12-12-01", "timestamp": 1704859921, "time_this_iter_s": 465.20488357543945, "time_total_s": 4398.83199429512, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710F113280>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 4398.83199429512, "timesteps_since_restore": 0, "iterations_since_restore": 9, "perf": {"cpu_util_percent": 17.60895295902883, "ram_util_percent": 62.60728376327769}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2216.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -554.0, "policy_1": -554.0, "policy_2": -554.0, "policy_3": -554.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -1600.0, -1600.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_1_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_2_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_3_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6878725966226727, "mean_inference_ms": 5.418209889707225, "mean_action_processing_ms": 0.13469805734056314, "mean_env_wait_ms": 9.610198567764321, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 200000, "timesteps_this_iter": 0, "agent_timesteps_total": 800000, "timers": {"sample_time_ms": 469547.481, "sample_throughput": 42.594, "load_time_ms": 1242.95, "load_throughput": 16090.746, "learn_time_ms": 173883.724, "learn_throughput": 115.019, "update_time_ms": 11.521}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 574.3448486328125, "cur_kl_coeff": 0.000390625, "cur_lr": 0.0004550000009000001, "total_loss": 2598.325439453125, "policy_loss": 1.8960952434809998e-07, "vf_loss": 2598.325439453125, "vf_explained_var": 1.5139579772949219e-05, "kl": -3.7768506933044674e-10, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 559.8486328125, "cur_kl_coeff": 0.000390625, "cur_lr": 0.0004550000009000001, "total_loss": 2597.151806640625, "policy_loss": 4.444313026263913e-07, "vf_loss": 2597.151806640625, "vf_explained_var": 1.1205673217773438e-05, "kl": 7.184287664774302e-10, "entropy": 3.218871593475342, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 561.6163330078125, "cur_kl_coeff": 0.000390625, "cur_lr": 0.0004550000009000001, "total_loss": 2598.76474609375, "policy_loss": 3.450679755268027e-07, "vf_loss": 2598.76474609375, "vf_explained_var": 4.035234451293945e-05, "kl": 1.205782140800693e-10, "entropy": 3.2188700675964355, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 581.7348022460938, "cur_kl_coeff": 0.000390625, "cur_lr": 0.0004550000009000001, "total_loss": 2596.769091796875, "policy_loss": 3.164482070605601e-07, "vf_loss": 2596.769091796875, "vf_explained_var": -3.0159950256347656e-05, "kl": 1.668231164320133e-10, "entropy": 3.2188720703125, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 200000, "num_agent_steps_sampled": 800000, "num_steps_trained": 200000, "num_agent_steps_trained": 800000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 100, "training_iteration": 10, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_12-19-47", "timestamp": 1704860387, "time_this_iter_s": 465.9956946372986, "time_total_s": 4864.827688932419, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A5C10>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 4864.827688932419, "timesteps_since_restore": 0, "iterations_since_restore": 10, "perf": {"cpu_util_percent": 17.695, "ram_util_percent": 64.96515151515152}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2212.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -553.0, "policy_1": -553.0, "policy_2": -553.0, "policy_3": -553.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -1600.0, -1600.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_1_reward": [-600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_2_reward": [-600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_3_reward": [-600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6857273449123268, "mean_inference_ms": 5.400374493060972, "mean_action_processing_ms": 0.13370752311070258, "mean_env_wait_ms": 9.594543788072759, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 220000, "timesteps_this_iter": 0, "agent_timesteps_total": 880000, "timers": {"sample_time_ms": 484289.497, "sample_throughput": 41.298, "load_time_ms": 1238.696, "load_throughput": 16146.018, "learn_time_ms": 173065.512, "learn_throughput": 115.563, "update_time_ms": 11.512}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 613.7391357421875, "cur_kl_coeff": 0.0001953125, "cur_lr": 0.000450000001, "total_loss": 2253.8802734375, "policy_loss": 2.173805223826264e-07, "vf_loss": 2253.8802734375, "vf_explained_var": 3.039836883544922e-06, "kl": -4.5156914041388775e-10, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 659.0626220703125, "cur_kl_coeff": 0.0001953125, "cur_lr": 0.000450000001, "total_loss": 2250.47177734375, "policy_loss": 4.497623544930107e-07, "vf_loss": 2250.47177734375, "vf_explained_var": 2.0623207092285156e-05, "kl": 1.147362033854238e-09, "entropy": 3.21887149810791, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 628.60693359375, "cur_kl_coeff": 0.0001953125, "cur_lr": 0.000450000001, "total_loss": 2255.265869140625, "policy_loss": 4.542255359840652e-07, "vf_loss": 2255.265869140625, "vf_explained_var": 2.5212764739990234e-05, "kl": -1.7439646948780307e-12, "entropy": 3.2188703536987306, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 616.7952880859375, "cur_kl_coeff": 0.0001953125, "cur_lr": 0.000450000001, "total_loss": 2252.054296875, "policy_loss": 1.2321472372539556e-07, "vf_loss": 2252.054296875, "vf_explained_var": -1.6689300537109375e-05, "kl": 1.5052018909056386e-09, "entropy": 3.2188720703125, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 220000, "num_agent_steps_sampled": 880000, "num_steps_trained": 220000, "num_agent_steps_trained": 880000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 110, "training_iteration": 11, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_12-27-31", "timestamp": 1704860851, "time_this_iter_s": 464.54217863082886, "time_total_s": 5329.369867563248, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A55E0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 5329.369867563248, "timesteps_since_restore": 0, "iterations_since_restore": 11, "perf": {"cpu_util_percent": 17.712917933130697, "ram_util_percent": 66.51762917933132}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2224.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -556.0, "policy_1": -556.0, "policy_2": -556.0, "policy_3": -556.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -1600.0, -1600.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0], "policy_policy_1_reward": [-500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0], "policy_policy_2_reward": [-500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0], "policy_policy_3_reward": [-500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6834208649429234, "mean_inference_ms": 5.383097995986663, "mean_action_processing_ms": 0.13333411027545752, "mean_env_wait_ms": 9.579861533524713, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 240000, "timesteps_this_iter": 0, "agent_timesteps_total": 960000, "timers": {"sample_time_ms": 481639.278, "sample_throughput": 41.525, "load_time_ms": 1237.546, "load_throughput": 16161.018, "learn_time_ms": 172097.898, "learn_throughput": 116.213, "update_time_ms": 11.412}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 561.9139404296875, "cur_kl_coeff": 9.765625e-05, "cur_lr": 0.00044500000110000006, "total_loss": 2593.50009765625, "policy_loss": 2.1349907448342532e-07, "vf_loss": 2593.50009765625, "vf_explained_var": 1.1920928955078125e-06, "kl": 1.1562859947300552e-09, "entropy": 3.218870162963867, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 577.330322265625, "cur_kl_coeff": 9.765625e-05, "cur_lr": 0.00044500000110000006, "total_loss": 2590.387744140625, "policy_loss": 4.334258960625448e-07, "vf_loss": 2590.387744140625, "vf_explained_var": 1.138448715209961e-05, "kl": 2.260062210990288e-09, "entropy": 3.218871593475342, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 564.9561767578125, "cur_kl_coeff": 9.765625e-05, "cur_lr": 0.00044500000110000006, "total_loss": 2596.083154296875, "policy_loss": 4.996585707850442e-07, "vf_loss": 2596.083154296875, "vf_explained_var": 3.057718276977539e-05, "kl": -1.1936031441006633e-09, "entropy": 3.21887001991272, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 552.2691650390625, "cur_kl_coeff": 9.765625e-05, "cur_lr": 0.00044500000110000006, "total_loss": 2599.064794921875, "policy_loss": 1.8218040773732013e-07, "vf_loss": 2599.064794921875, "vf_explained_var": -8.702278137207031e-06, "kl": -8.671330802706434e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 240000, "num_agent_steps_sampled": 960000, "num_steps_trained": 240000, "num_agent_steps_trained": 960000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 120, "training_iteration": 12, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_12-35-17", "timestamp": 1704861317, "time_this_iter_s": 465.35485887527466, "time_total_s": 5794.724726438522, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170F9AB5F70>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 5794.724726438522, "timesteps_since_restore": 0, "iterations_since_restore": 12, "perf": {"cpu_util_percent": 17.730349013657055, "ram_util_percent": 68.85553869499242}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2216.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -554.0, "policy_1": -554.0, "policy_2": -554.0, "policy_3": -554.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -1600.0, -1600.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_1_reward": [-500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_2_reward": [-500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_3_reward": [-500.0, -400.0, -400.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.679902790789165, "mean_inference_ms": 5.357561664307461, "mean_action_processing_ms": 0.13290627257937648, "mean_env_wait_ms": 9.553671375578963, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 260000, "timesteps_this_iter": 0, "agent_timesteps_total": 1040000, "timers": {"sample_time_ms": 477771.606, "sample_throughput": 41.861, "load_time_ms": 1234.243, "load_throughput": 16204.264, "learn_time_ms": 171397.397, "learn_throughput": 116.688, "update_time_ms": 11.228}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 493.74755859375, "cur_kl_coeff": 4.8828125e-05, "cur_lr": 0.0004400000012, "total_loss": 2477.8794921875, "policy_loss": 2.3720741317845296e-07, "vf_loss": 2477.8794921875, "vf_explained_var": -1.7881393432617188e-06, "kl": -2.1678766892740954e-09, "entropy": 3.2188690662384034, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 539.2694091796875, "cur_kl_coeff": 4.8828125e-05, "cur_lr": 0.0004400000012, "total_loss": 2470.329248046875, "policy_loss": 2.574920627829158e-07, "vf_loss": 2470.329248046875, "vf_explained_var": 3.7550926208496094e-06, "kl": 1.7792039230801748e-09, "entropy": 3.218870687484741, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 562.6671142578125, "cur_kl_coeff": 4.8828125e-05, "cur_lr": 0.0004400000012, "total_loss": 2484.1498046875, "policy_loss": 1.9443512471184478e-07, "vf_loss": 2484.1498046875, "vf_explained_var": 2.4199485778808594e-05, "kl": -5.523021474190814e-10, "entropy": 3.218870162963867, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 574.6238403320312, "cur_kl_coeff": 4.8828125e-05, "cur_lr": 0.0004400000012, "total_loss": 2484.646826171875, "policy_loss": 1.6387939257844764e-07, "vf_loss": 2484.646826171875, "vf_explained_var": -4.0411949157714844e-05, "kl": 1.532858978636753e-09, "entropy": 3.218871593475342, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 260000, "num_agent_steps_sampled": 1040000, "num_steps_trained": 260000, "num_agent_steps_trained": 1040000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 130, "training_iteration": 13, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_12-43-03", "timestamp": 1704861783, "time_this_iter_s": 466.28724193573, "time_total_s": 6261.011968374252, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x0000017102DA3A60>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 6261.011968374252, "timesteps_since_restore": 0, "iterations_since_restore": 13, "perf": {"cpu_util_percent": 17.72375189107413, "ram_util_percent": 70.46959152798789}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2228.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -557.0, "policy_1": -557.0, "policy_2": -557.0, "policy_3": -557.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0], "policy_policy_1_reward": [-600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0], "policy_policy_2_reward": [-600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0], "policy_policy_3_reward": [-600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6753092946960311, "mean_inference_ms": 5.321889135327051, "mean_action_processing_ms": 0.13232082870482045, "mean_env_wait_ms": 9.51381605027069, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 280000, "timesteps_this_iter": 0, "agent_timesteps_total": 1120000, "timers": {"sample_time_ms": 473104.92, "sample_throughput": 42.274, "load_time_ms": 1232.3, "load_throughput": 16229.815, "learn_time_ms": 170283.235, "learn_throughput": 117.451, "update_time_ms": 11.151}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 620.232421875, "cur_kl_coeff": 2.44140625e-05, "cur_lr": 0.0004350000013, "total_loss": 2398.140673828125, "policy_loss": 4.3320656040179983e-07, "vf_loss": 2398.140673828125, "vf_explained_var": 1.3649463653564453e-05, "kl": -5.974291755750372e-11, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 596.9613647460938, "cur_kl_coeff": 2.44140625e-05, "cur_lr": 0.0004350000013, "total_loss": 2387.662158203125, "policy_loss": 3.0830383588309476e-07, "vf_loss": 2387.662158203125, "vf_explained_var": -8.463859558105469e-06, "kl": 8.373430615549182e-10, "entropy": 3.2188714504241944, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 663.12646484375, "cur_kl_coeff": 2.44140625e-05, "cur_lr": 0.0004350000013, "total_loss": 2400.5388671875, "policy_loss": 2.1784782191502928e-07, "vf_loss": 2400.5388671875, "vf_explained_var": 1.901388168334961e-05, "kl": 1.0695784195824398e-09, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 641.0965576171875, "cur_kl_coeff": 2.44140625e-05, "cur_lr": 0.0004350000013, "total_loss": 2407.075146484375, "policy_loss": 2.5072097624700974e-07, "vf_loss": 2407.075146484375, "vf_explained_var": -8.106231689453125e-06, "kl": -2.2486535099230932e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 280000, "num_agent_steps_sampled": 1120000, "num_steps_trained": 280000, "num_agent_steps_trained": 1120000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 140, "training_iteration": 14, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_12-50-49", "timestamp": 1704862249, "time_this_iter_s": 465.8883969783783, "time_total_s": 6726.900365352631, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x0000017102DA30D0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 6726.900365352631, "timesteps_since_restore": 0, "iterations_since_restore": 14, "perf": {"cpu_util_percent": 17.723333333333333, "ram_util_percent": 73.19530303030304}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2244.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -561.0, "policy_1": -561.0, "policy_2": -561.0, "policy_3": -561.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_1_reward": [-500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_2_reward": [-500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_3_reward": [-500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6703619373302613, "mean_inference_ms": 5.283628160153672, "mean_action_processing_ms": 0.13172709300495225, "mean_env_wait_ms": 9.468886771389137, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 300000, "timesteps_this_iter": 0, "agent_timesteps_total": 1200000, "timers": {"sample_time_ms": 468979.847, "sample_throughput": 42.646, "load_time_ms": 1230.634, "load_throughput": 16251.791, "learn_time_ms": 168936.678, "learn_throughput": 118.388, "update_time_ms": 11.074}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 691.1066284179688, "cur_kl_coeff": 1.220703125e-05, "cur_lr": 0.0004300000014, "total_loss": 2620.492578125, "policy_loss": 4.903507116571504e-07, "vf_loss": 2620.492578125, "vf_explained_var": -1.33514404296875e-05, "kl": -1.1873727612243813e-09, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 739.1807861328125, "cur_kl_coeff": 1.220703125e-05, "cur_lr": 0.0004300000014, "total_loss": 2614.79794921875, "policy_loss": 1.6620636746900175e-07, "vf_loss": 2614.79794921875, "vf_explained_var": 7.62939453125e-06, "kl": -9.883375257091487e-10, "entropy": 3.2188705921173097, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 624.5845336914062, "cur_kl_coeff": 1.220703125e-05, "cur_lr": 0.0004300000014, "total_loss": 2620.217138671875, "policy_loss": 3.249836025176478e-07, "vf_loss": 2620.217138671875, "vf_explained_var": 8.046627044677734e-06, "kl": 1.6203385494462098e-09, "entropy": 3.2188698291778564, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 648.5413818359375, "cur_kl_coeff": 1.220703125e-05, "cur_lr": 0.0004300000014, "total_loss": 2615.5953125, "policy_loss": 3.148269577479823e-07, "vf_loss": 2615.5953125, "vf_explained_var": -1.5735626220703125e-05, "kl": -4.4326025870622397e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 300000, "num_agent_steps_sampled": 1200000, "num_steps_trained": 300000, "num_agent_steps_trained": 1200000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 150, "training_iteration": 15, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_12-58-36", "timestamp": 1704862716, "time_this_iter_s": 466.9534845352173, "time_total_s": 7193.853849887848, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170F9AB5F70>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 7193.853849887848, "timesteps_since_restore": 0, "iterations_since_restore": 15, "perf": {"cpu_util_percent": 17.678971255673222, "ram_util_percent": 73.77700453857793}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2252.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -563.0, "policy_1": -563.0, "policy_2": -563.0, "policy_3": -563.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0], "policy_policy_1_reward": [-500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0], "policy_policy_2_reward": [-500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0], "policy_policy_3_reward": [-500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6659719277322769, "mean_inference_ms": 5.249637024593421, "mean_action_processing_ms": 0.13115063932865945, "mean_env_wait_ms": 9.430066352027152, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 320000, "timesteps_this_iter": 0, "agent_timesteps_total": 1280000, "timers": {"sample_time_ms": 466700.135, "sample_throughput": 42.854, "load_time_ms": 1228.339, "load_throughput": 16282.145, "learn_time_ms": 168301.659, "learn_throughput": 118.834, "update_time_ms": 11.066}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 660.8238525390625, "cur_kl_coeff": 6.103515625e-06, "cur_lr": 0.00042500000149999997, "total_loss": 2519.131396484375, "policy_loss": 2.2469520968293467e-07, "vf_loss": 2519.131396484375, "vf_explained_var": 2.86102294921875e-06, "kl": 7.427508512750092e-10, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 676.5079956054688, "cur_kl_coeff": 6.103515625e-06, "cur_lr": 0.00042500000149999997, "total_loss": 2509.310986328125, "policy_loss": 2.0696640437023462e-07, "vf_loss": 2509.310986328125, "vf_explained_var": 1.9252300262451172e-05, "kl": 1.1611393235799738e-09, "entropy": 3.218871593475342, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 750.6497802734375, "cur_kl_coeff": 6.103515625e-06, "cur_lr": 0.00042500000149999997, "total_loss": 2530.09619140625, "policy_loss": 2.506732906581988e-07, "vf_loss": 2530.09619140625, "vf_explained_var": 2.1159648895263672e-05, "kl": -1.1606368921501798e-10, "entropy": 3.2188696384429933, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 665.7316284179688, "cur_kl_coeff": 6.103515625e-06, "cur_lr": 0.00042500000149999997, "total_loss": 2521.393115234375, "policy_loss": 3.350448621652902e-07, "vf_loss": 2521.393115234375, "vf_explained_var": -2.9087066650390625e-05, "kl": -2.7360480547855558e-11, "entropy": 3.2188720703125, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 320000, "num_agent_steps_sampled": 1280000, "num_steps_trained": 320000, "num_agent_steps_trained": 1280000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 160, "training_iteration": 16, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_13-06-23", "timestamp": 1704863183, "time_this_iter_s": 467.23576760292053, "time_total_s": 7661.089617490768, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A5AF0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 7661.089617490768, "timesteps_since_restore": 0, "iterations_since_restore": 16, "perf": {"cpu_util_percent": 17.637915407854983, "ram_util_percent": 75.31132930513596}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2260.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -565.0, "policy_1": -565.0, "policy_2": -565.0, "policy_3": -565.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_1_reward": [-500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_2_reward": [-500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_3_reward": [-500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6625207638326954, "mean_inference_ms": 5.222211218187798, "mean_action_processing_ms": 0.13066955460550286, "mean_env_wait_ms": 9.398591871119462, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 340000, "timesteps_this_iter": 0, "agent_timesteps_total": 1360000, "timers": {"sample_time_ms": 465997.239, "sample_throughput": 42.919, "load_time_ms": 1225.933, "load_throughput": 16314.11, "learn_time_ms": 168143.03, "learn_throughput": 118.946, "update_time_ms": 10.866}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 701.5621337890625, "cur_kl_coeff": 3.0517578125e-06, "cur_lr": 0.0004200000016, "total_loss": 2445.666748046875, "policy_loss": 7.6599120246712e-08, "vf_loss": 2445.666748046875, "vf_explained_var": -1.704692840576172e-05, "kl": 3.384176013032647e-09, "entropy": 3.2188690662384034, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 753.7638549804688, "cur_kl_coeff": 3.0517578125e-06, "cur_lr": 0.0004200000016, "total_loss": 2429.201611328125, "policy_loss": 3.832340301546111e-07, "vf_loss": 2429.201611328125, "vf_explained_var": 1.6808509826660156e-05, "kl": 4.847306875332969e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 641.3553466796875, "cur_kl_coeff": 3.0517578125e-06, "cur_lr": 0.0004200000016, "total_loss": 2436.03515625, "policy_loss": 2.434349054780682e-07, "vf_loss": 2436.03515625, "vf_explained_var": 3.361701965332031e-05, "kl": 9.530101374721766e-10, "entropy": 3.218870162963867, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 857.04833984375, "cur_kl_coeff": 3.0517578125e-06, "cur_lr": 0.0004200000016, "total_loss": 2444.498291015625, "policy_loss": 1.5508651620699255e-07, "vf_loss": 2444.498291015625, "vf_explained_var": -3.5762786865234375e-07, "kl": 7.605032714685933e-10, "entropy": 3.2188720226287844, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 340000, "num_agent_steps_sampled": 1360000, "num_steps_trained": 340000, "num_agent_steps_trained": 1360000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 170, "training_iteration": 17, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_13-14-07", "timestamp": 1704863647, "time_this_iter_s": 463.95971488952637, "time_total_s": 8125.049332380295, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x0000017129225AF0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 8125.049332380295, "timesteps_since_restore": 0, "iterations_since_restore": 17, "perf": {"cpu_util_percent": 17.610654490106548, "ram_util_percent": 76.88721461187215}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2252.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -563.0, "policy_1": -563.0, "policy_2": -563.0, "policy_3": -563.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_1_reward": [-600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_2_reward": [-600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_3_reward": [-600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6598510643518415, "mean_inference_ms": 5.199671498103345, "mean_action_processing_ms": 0.13024478281138094, "mean_env_wait_ms": 9.37267434336589, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 360000, "timesteps_this_iter": 0, "agent_timesteps_total": 1440000, "timers": {"sample_time_ms": 466021.082, "sample_throughput": 42.917, "load_time_ms": 1221.879, "load_throughput": 16368.227, "learn_time_ms": 167990.497, "learn_throughput": 119.054, "update_time_ms": 10.858}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 820.0980224609375, "cur_kl_coeff": 1.52587890625e-06, "cur_lr": 0.00041500000170000006, "total_loss": 2228.826904296875, "policy_loss": 3.99818418994613e-07, "vf_loss": 2228.826904296875, "vf_explained_var": 1.2516975402832031e-05, "kl": 3.822330785219208e-10, "entropy": 3.2188690662384034, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 801.677490234375, "cur_kl_coeff": 1.52587890625e-06, "cur_lr": 0.00041500000170000006, "total_loss": 2225.70166015625, "policy_loss": 1.4146804894199505e-07, "vf_loss": 2225.70166015625, "vf_explained_var": 2.2351741790771484e-05, "kl": 7.227672904619453e-11, "entropy": 3.218871593475342, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 749.0911865234375, "cur_kl_coeff": 1.52587890625e-06, "cur_lr": 0.00041500000170000006, "total_loss": 2223.873876953125, "policy_loss": 4.701328267131544e-07, "vf_loss": 2223.873876953125, "vf_explained_var": 2.1517276763916016e-05, "kl": 3.1616753402852194e-09, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 800.1008911132812, "cur_kl_coeff": 1.52587890625e-06, "cur_lr": 0.00041500000170000006, "total_loss": 2219.600390625, "policy_loss": 2.2954941094610604e-07, "vf_loss": 2219.600390625, "vf_explained_var": 1.430511474609375e-06, "kl": 1.2739366367364724e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 360000, "num_agent_steps_sampled": 1440000, "num_steps_trained": 360000, "num_agent_steps_trained": 1440000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 180, "training_iteration": 18, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_13-21-54", "timestamp": 1704864114, "time_this_iter_s": 466.7154312133789, "time_total_s": 8591.764763593674, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170441CF040>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 8591.764763593674, "timesteps_since_restore": 0, "iterations_since_restore": 18, "perf": {"cpu_util_percent": 17.630408472012103, "ram_util_percent": 78.89984871406959}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2232.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -558.0, "policy_1": -558.0, "policy_2": -558.0, "policy_3": -558.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2000.0, -1600.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0], "policy_policy_1_reward": [-600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0], "policy_policy_2_reward": [-600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0], "policy_policy_3_reward": [-600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6576797378510464, "mean_inference_ms": 5.1806949116480885, "mean_action_processing_ms": 0.1298348520446426, "mean_env_wait_ms": 9.351313118556035, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 380000, "timesteps_this_iter": 0, "agent_timesteps_total": 1520000, "timers": {"sample_time_ms": 465959.366, "sample_throughput": 42.922, "load_time_ms": 1226.757, "load_throughput": 16303.147, "learn_time_ms": 167905.878, "learn_throughput": 119.114, "update_time_ms": 10.858}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 650.6693115234375, "cur_kl_coeff": 7.62939453125e-07, "cur_lr": 0.00041000000180000003, "total_loss": 2075.049951171875, "policy_loss": 1.7902374302991574e-07, "vf_loss": 2075.049951171875, "vf_explained_var": -5.960464477539062e-07, "kl": 6.334826108123081e-10, "entropy": 3.218870162963867, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 730.5169067382812, "cur_kl_coeff": 7.62939453125e-07, "cur_lr": 0.00041000000180000003, "total_loss": 2069.975732421875, "policy_loss": 2.045249971160956e-07, "vf_loss": 2069.975732421875, "vf_explained_var": 1.430511474609375e-05, "kl": 3.192721920175501e-11, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 729.6460571289062, "cur_kl_coeff": 7.62939453125e-07, "cur_lr": 0.00041000000180000003, "total_loss": 2074.577783203125, "policy_loss": 3.095912880946017e-07, "vf_loss": 2074.577783203125, "vf_explained_var": 7.867813110351562e-06, "kl": -1.7421728226718613e-10, "entropy": 3.2188700675964355, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 759.677734375, "cur_kl_coeff": 7.62939453125e-07, "cur_lr": 0.00041000000180000003, "total_loss": 2083.406103515625, "policy_loss": 2.400016764214641e-07, "vf_loss": 2083.406103515625, "vf_explained_var": -1.4901161193847656e-05, "kl": 6.577069250335299e-10, "entropy": 3.2188720703125, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 380000, "num_agent_steps_sampled": 1520000, "num_steps_trained": 380000, "num_agent_steps_trained": 1520000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 190, "training_iteration": 19, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_13-29-40", "timestamp": 1704864580, "time_this_iter_s": 465.35410952568054, "time_total_s": 9057.118873119354, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170443ACD30>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 9057.118873119354, "timesteps_since_restore": 0, "iterations_since_restore": 19, "perf": {"cpu_util_percent": 17.787537993920974, "ram_util_percent": 80.14832826747721}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2232.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -558.0, "policy_1": -558.0, "policy_2": -558.0, "policy_3": -558.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2000.0, -1600.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0], "policy_policy_1_reward": [-500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0], "policy_policy_2_reward": [-500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0], "policy_policy_3_reward": [-500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6557625970941069, "mean_inference_ms": 5.164468456753589, "mean_action_processing_ms": 0.12948408591318927, "mean_env_wait_ms": 9.333252325830978, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 400000, "timesteps_this_iter": 0, "agent_timesteps_total": 1600000, "timers": {"sample_time_ms": 465878.422, "sample_throughput": 42.93, "load_time_ms": 1225.956, "load_throughput": 16313.796, "learn_time_ms": 167757.526, "learn_throughput": 119.22, "update_time_ms": 10.977}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 772.0396728515625, "cur_kl_coeff": 3.814697265625e-07, "cur_lr": 0.0004050000019, "total_loss": 2602.029541015625, "policy_loss": 2.7523040424526357e-07, "vf_loss": 2602.029541015625, "vf_explained_var": 7.68899917602539e-06, "kl": 1.4609240449325879e-10, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 848.8952026367188, "cur_kl_coeff": 3.814697265625e-07, "cur_lr": 0.0004050000019, "total_loss": 2592.4970703125, "policy_loss": 2.761077920965249e-07, "vf_loss": 2592.4970703125, "vf_explained_var": 8.940696716308594e-06, "kl": 3.5698719536014776e-10, "entropy": 3.2188705921173097, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1006.7507934570312, "cur_kl_coeff": 3.814697265625e-07, "cur_lr": 0.0004050000019, "total_loss": 2607.473681640625, "policy_loss": 2.491474112353842e-07, "vf_loss": 2607.473681640625, "vf_explained_var": 2.0325183868408203e-05, "kl": 2.6989196988402854e-11, "entropy": 3.2188699722290037, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 731.048583984375, "cur_kl_coeff": 3.814697265625e-07, "cur_lr": 0.0004050000019, "total_loss": 2600.198583984375, "policy_loss": 4.784870230700733e-07, "vf_loss": 2600.198583984375, "vf_explained_var": -6.079673767089844e-06, "kl": 1.6079478026842509e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 400000, "num_agent_steps_sampled": 1600000, "num_steps_trained": 400000, "num_agent_steps_trained": 1600000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 200, "training_iteration": 20, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_13-37-24", "timestamp": 1704865044, "time_this_iter_s": 464.479496717453, "time_total_s": 9521.598369836807, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A5E50>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 9521.598369836807, "timesteps_since_restore": 0, "iterations_since_restore": 20, "perf": {"cpu_util_percent": 17.712006079027354, "ram_util_percent": 81.15881458966565}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2244.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -561.0, "policy_1": -561.0, "policy_2": -561.0, "policy_3": -561.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2000.0, -1600.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0], "policy_policy_1_reward": [-600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0], "policy_policy_2_reward": [-600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0], "policy_policy_3_reward": [-600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6540381891250517, "mean_inference_ms": 5.1505511464909866, "mean_action_processing_ms": 0.12917054222271612, "mean_env_wait_ms": 9.318585985452962, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 420000, "timesteps_this_iter": 0, "agent_timesteps_total": 1680000, "timers": {"sample_time_ms": 465968.958, "sample_throughput": 42.921, "load_time_ms": 1225.589, "load_throughput": 16318.686, "learn_time_ms": 167617.481, "learn_throughput": 119.319, "update_time_ms": 10.994}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 662.1573486328125, "cur_kl_coeff": 1.9073486328125e-07, "cur_lr": 0.000400000002, "total_loss": 2569.638720703125, "policy_loss": 1.5143394381800236e-07, "vf_loss": 2569.638720703125, "vf_explained_var": 1.6093254089355469e-06, "kl": -3.5617689075895244e-10, "entropy": 3.2188690185546873, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 739.9263916015625, "cur_kl_coeff": 1.9073486328125e-07, "cur_lr": 0.000400000002, "total_loss": 2562.369970703125, "policy_loss": 2.9126167249149405e-07, "vf_loss": 2562.369970703125, "vf_explained_var": 1.9848346710205078e-05, "kl": -5.734266965776169e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 761.3380737304688, "cur_kl_coeff": 1.9073486328125e-07, "cur_lr": 0.000400000002, "total_loss": 2583.158642578125, "policy_loss": 3.416347520435181e-07, "vf_loss": 2583.158642578125, "vf_explained_var": 1.7404556274414062e-05, "kl": 1.023316134607377e-09, "entropy": 3.2188693046569825, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 681.4906005859375, "cur_kl_coeff": 1.9073486328125e-07, "cur_lr": 0.000400000002, "total_loss": 2563.0037109375, "policy_loss": 2.5235175709070746e-07, "vf_loss": 2563.0037109375, "vf_explained_var": -2.1457672119140625e-06, "kl": 1.4861930804077606e-09, "entropy": 3.2188720703125, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 420000, "num_agent_steps_sampled": 1680000, "num_steps_trained": 420000, "num_agent_steps_trained": 1680000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 210, "training_iteration": 21, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_13-45-10", "timestamp": 1704865510, "time_this_iter_s": 465.5377390384674, "time_total_s": 9987.136108875275, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170FEE90940>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 9987.136108875275, "timesteps_since_restore": 0, "iterations_since_restore": 21, "perf": {"cpu_util_percent": 17.653414264036417, "ram_util_percent": 82.848406676783}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2240.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -560.0, "policy_1": -560.0, "policy_2": -560.0, "policy_3": -560.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2000.0, -1600.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_1_reward": [-500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_2_reward": [-500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_3_reward": [-500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.652641536533522, "mean_inference_ms": 5.138414895219596, "mean_action_processing_ms": 0.1288942173192079, "mean_env_wait_ms": 9.306173072673722, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 440000, "timesteps_this_iter": 0, "agent_timesteps_total": 1760000, "timers": {"sample_time_ms": 465982.192, "sample_throughput": 42.92, "load_time_ms": 1223.79, "load_throughput": 16342.677, "learn_time_ms": 167475.505, "learn_throughput": 119.42, "update_time_ms": 11.094}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 809.1099243164062, "cur_kl_coeff": 9.5367431640625e-08, "cur_lr": 0.0003950000021000001, "total_loss": 2603.88779296875, "policy_loss": 3.161907223159233e-07, "vf_loss": 2603.88779296875, "vf_explained_var": 9.417533874511719e-06, "kl": 1.2806046903457436e-09, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 662.7923583984375, "cur_kl_coeff": 9.5367431640625e-08, "cur_lr": 0.0003950000021000001, "total_loss": 2599.99501953125, "policy_loss": 9.024620098996294e-08, "vf_loss": 2599.99501953125, "vf_explained_var": 6.318092346191406e-06, "kl": 6.815442397944959e-10, "entropy": 3.2188705921173097, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 703.6466064453125, "cur_kl_coeff": 9.5367431640625e-08, "cur_lr": 0.0003950000021000001, "total_loss": 2605.096337890625, "policy_loss": 2.2735595148759557e-07, "vf_loss": 2605.096337890625, "vf_explained_var": 2.5093555450439453e-05, "kl": 2.3883555311417226e-10, "entropy": 3.218869686126709, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 657.8004150390625, "cur_kl_coeff": 9.5367431640625e-08, "cur_lr": 0.0003950000021000001, "total_loss": 2608.572900390625, "policy_loss": 2.9428481216342563e-07, "vf_loss": 2608.572900390625, "vf_explained_var": -2.2649765014648438e-05, "kl": 5.041405813643874e-10, "entropy": 3.218871593475342, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 440000, "num_agent_steps_sampled": 1760000, "num_steps_trained": 440000, "num_agent_steps_trained": 1760000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 220, "training_iteration": 22, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_13-52-55", "timestamp": 1704865975, "time_this_iter_s": 465.46620893478394, "time_total_s": 10452.602317810059, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170441CF040>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 10452.602317810059, "timesteps_since_restore": 0, "iterations_since_restore": 22, "perf": {"cpu_util_percent": 17.655842185128986, "ram_util_percent": 84.11911987860394}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2236.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -559.0, "policy_1": -559.0, "policy_2": -559.0, "policy_3": -559.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2000.0, -2400.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2000.0, -1600.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0], "policy_policy_1_reward": [-600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0], "policy_policy_2_reward": [-600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0], "policy_policy_3_reward": [-600.0, -500.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.651320644445537, "mean_inference_ms": 5.127698042728878, "mean_action_processing_ms": 0.12863800939621597, "mean_env_wait_ms": 9.295317010878343, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 460000, "timesteps_this_iter": 0, "agent_timesteps_total": 1840000, "timers": {"sample_time_ms": 465802.774, "sample_throughput": 42.937, "load_time_ms": 1226.83, "load_throughput": 16302.178, "learn_time_ms": 167385.375, "learn_throughput": 119.485, "update_time_ms": 11.08}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 728.83251953125, "cur_kl_coeff": 4.76837158203125e-08, "cur_lr": 0.0003900000022, "total_loss": 2527.304296875, "policy_loss": 2.3749351885093972e-07, "vf_loss": 2527.304296875, "vf_explained_var": -9.775161743164062e-06, "kl": -3.776859214266182e-10, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 721.9509887695312, "cur_kl_coeff": 4.76837158203125e-08, "cur_lr": 0.0003900000022, "total_loss": 2519.860693359375, "policy_loss": 3.560829173077096e-07, "vf_loss": 2519.860693359375, "vf_explained_var": 1.2636184692382812e-05, "kl": 4.419366428365379e-10, "entropy": 3.2188707828521728, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 800.7862548828125, "cur_kl_coeff": 4.76837158203125e-08, "cur_lr": 0.0003900000022, "total_loss": 2530.16513671875, "policy_loss": 3.3688545344467966e-07, "vf_loss": 2530.16513671875, "vf_explained_var": 1.3530254364013672e-05, "kl": -6.373019889593934e-10, "entropy": 3.218869924545288, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 709.8096923828125, "cur_kl_coeff": 4.76837158203125e-08, "cur_lr": 0.0003900000022, "total_loss": 2529.2548828125, "policy_loss": 2.896499571525268e-07, "vf_loss": 2529.2548828125, "vf_explained_var": -4.0531158447265625e-06, "kl": -8.438577614455766e-12, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 460000, "num_agent_steps_sampled": 1840000, "num_steps_trained": 460000, "num_agent_steps_trained": 1840000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 230, "training_iteration": 23, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_14-00-40", "timestamp": 1704866440, "time_this_iter_s": 465.0505349636078, "time_total_s": 10917.652852773666, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A53A0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 10917.652852773666, "timesteps_since_restore": 0, "iterations_since_restore": 23, "perf": {"cpu_util_percent": 17.652655538694994, "ram_util_percent": 84.0206373292868}}
{"episode_reward_max": -1200.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2236.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -300.0, "policy_1": -300.0, "policy_2": -300.0, "policy_3": -300.0}, "policy_reward_mean": {"policy_0": -559.0, "policy_1": -559.0, "policy_2": -559.0, "policy_3": -559.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2000.0, -1600.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1200.0, -2000.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0], "policy_policy_1_reward": [-500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0], "policy_policy_2_reward": [-500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0], "policy_policy_3_reward": [-500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6501605879916519, "mean_inference_ms": 5.118141516923352, "mean_action_processing_ms": 0.12840173572437436, "mean_env_wait_ms": 9.286459832118469, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 480000, "timesteps_this_iter": 0, "agent_timesteps_total": 1920000, "timers": {"sample_time_ms": 466019.293, "sample_throughput": 42.917, "load_time_ms": 1223.196, "load_throughput": 16350.604, "learn_time_ms": 167279.314, "learn_throughput": 119.561, "update_time_ms": 11.189}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 730.0555419921875, "cur_kl_coeff": 2.384185791015625e-08, "cur_lr": 0.0003850000023, "total_loss": 2290.57626953125, "policy_loss": 3.238391964366372e-07, "vf_loss": 2290.57626953125, "vf_explained_var": 4.172325134277344e-06, "kl": 1.4470112115472223e-09, "entropy": 3.218868923187256, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 840.0162353515625, "cur_kl_coeff": 2.384185791015625e-08, "cur_lr": 0.0003850000023, "total_loss": 2288.0921875, "policy_loss": 3.506469697711623e-07, "vf_loss": 2288.0921875, "vf_explained_var": -1.7881393432617188e-06, "kl": 1.2353409769086387e-09, "entropy": 3.218871259689331, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 884.9466552734375, "cur_kl_coeff": 2.384185791015625e-08, "cur_lr": 0.0003850000023, "total_loss": 2300.919580078125, "policy_loss": 3.841972379681202e-07, "vf_loss": 2300.919580078125, "vf_explained_var": 1.5735626220703125e-05, "kl": 4.103190093251996e-10, "entropy": 3.21886944770813, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 884.1234130859375, "cur_kl_coeff": 2.384185791015625e-08, "cur_lr": 0.0003850000023, "total_loss": 2298.303759765625, "policy_loss": 2.689743118899912e-07, "vf_loss": 2298.303759765625, "vf_explained_var": -3.0159950256347656e-05, "kl": -4.427483935032228e-11, "entropy": 3.2188718795776365, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 480000, "num_agent_steps_sampled": 1920000, "num_steps_trained": 480000, "num_agent_steps_trained": 1920000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 240, "training_iteration": 24, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_14-08-28", "timestamp": 1704866908, "time_this_iter_s": 467.81253838539124, "time_total_s": 11385.465391159058, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170F9AB5F70>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 11385.465391159058, "timesteps_since_restore": 0, "iterations_since_restore": 24, "perf": {"cpu_util_percent": 17.657099697885197, "ram_util_percent": 81.64924471299094}}
{"episode_reward_max": -1200.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2216.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -300.0, "policy_1": -300.0, "policy_2": -300.0, "policy_3": -300.0}, "policy_reward_mean": {"policy_0": -554.0, "policy_1": -554.0, "policy_2": -554.0, "policy_3": -554.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2000.0, -1600.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1200.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0], "policy_policy_1_reward": [-600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0], "policy_policy_2_reward": [-600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0], "policy_policy_3_reward": [-600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6491334294350125, "mean_inference_ms": 5.109509591295355, "mean_action_processing_ms": 0.12816121248925813, "mean_env_wait_ms": 9.278375559136755, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 500000, "timesteps_this_iter": 0, "agent_timesteps_total": 2000000, "timers": {"sample_time_ms": 465833.625, "sample_throughput": 42.934, "load_time_ms": 1221.828, "load_throughput": 16368.915, "learn_time_ms": 167264.926, "learn_throughput": 119.571, "update_time_ms": 11.263}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 871.9864501953125, "cur_kl_coeff": 1.1920928955078126e-08, "cur_lr": 0.00038000000240000003, "total_loss": 2154.21455078125, "policy_loss": 3.387451205760783e-07, "vf_loss": 2154.21455078125, "vf_explained_var": -8.344650268554688e-07, "kl": -6.044808861793704e-10, "entropy": 3.218869352340698, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 878.2940673828125, "cur_kl_coeff": 1.1920928955078126e-08, "cur_lr": 0.00038000000240000003, "total_loss": 2159.27060546875, "policy_loss": 3.517341601977364e-07, "vf_loss": 2159.27060546875, "vf_explained_var": 1.6927719116210938e-05, "kl": -7.840651267132515e-10, "entropy": 3.218871593475342, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 860.2919921875, "cur_kl_coeff": 1.1920928955078126e-08, "cur_lr": 0.00038000000240000003, "total_loss": 2175.141015625, "policy_loss": 2.4381638035197995e-07, "vf_loss": 2175.141015625, "vf_explained_var": 4.0650367736816406e-05, "kl": 8.193783785151454e-10, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 995.44873046875, "cur_kl_coeff": 1.1920928955078126e-08, "cur_lr": 0.00038000000240000003, "total_loss": 2179.7640625, "policy_loss": 1.9760132022872411e-07, "vf_loss": 2179.7640625, "vf_explained_var": -1.3589859008789062e-05, "kl": -2.377960023336456e-09, "entropy": 3.218871593475342, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 500000, "num_agent_steps_sampled": 2000000, "num_steps_trained": 500000, "num_agent_steps_trained": 2000000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 250, "training_iteration": 25, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_14-16-14", "timestamp": 1704867374, "time_this_iter_s": 466.05884313583374, "time_total_s": 11851.524234294891, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x0000017102DA3AF0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 11851.524234294891, "timesteps_since_restore": 0, "iterations_since_restore": 25, "perf": {"cpu_util_percent": 17.693797276853253, "ram_util_percent": 84.8251134644478}}
{"episode_reward_max": -1200.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2216.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -300.0, "policy_1": -300.0, "policy_2": -300.0, "policy_3": -300.0}, "policy_reward_mean": {"policy_0": -554.0, "policy_1": -554.0, "policy_2": -554.0, "policy_3": -554.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2000.0, -1600.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1200.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0], "policy_policy_1_reward": [-600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0], "policy_policy_2_reward": [-600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0], "policy_policy_3_reward": [-600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6482424587216278, "mean_inference_ms": 5.101669902962858, "mean_action_processing_ms": 0.12795379273477347, "mean_env_wait_ms": 9.270841941543175, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 520000, "timesteps_this_iter": 0, "agent_timesteps_total": 2080000, "timers": {"sample_time_ms": 465772.578, "sample_throughput": 42.939, "load_time_ms": 1221.821, "load_throughput": 16369.008, "learn_time_ms": 167255.953, "learn_throughput": 119.577, "update_time_ms": 11.263}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 769.7984008789062, "cur_kl_coeff": 5.960464477539063e-09, "cur_lr": 0.0003750000025, "total_loss": 2216.210791015625, "policy_loss": 2.890396113175431e-07, "vf_loss": 2216.210791015625, "vf_explained_var": 8.821487426757812e-06, "kl": 6.209497499520822e-10, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 849.1302490234375, "cur_kl_coeff": 5.960464477539063e-09, "cur_lr": 0.0003750000025, "total_loss": 2207.2419921875, "policy_loss": 1.743888833338758e-07, "vf_loss": 2207.2419921875, "vf_explained_var": 9.298324584960938e-06, "kl": -9.23372461825167e-11, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 745.0311889648438, "cur_kl_coeff": 5.960464477539063e-09, "cur_lr": 0.0003750000025, "total_loss": 2214.4375, "policy_loss": 2.2701264008606613e-07, "vf_loss": 2214.4375, "vf_explained_var": 2.0563602447509766e-05, "kl": -3.749621468873699e-09, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 780.8106689453125, "cur_kl_coeff": 5.960464477539063e-09, "cur_lr": 0.0003750000025, "total_loss": 2211.477099609375, "policy_loss": 3.3253669675659123e-07, "vf_loss": 2211.477099609375, "vf_explained_var": -1.4662742614746094e-05, "kl": 5.909718076013704e-10, "entropy": 3.2188720703125, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 520000, "num_agent_steps_sampled": 2080000, "num_steps_trained": 520000, "num_agent_steps_trained": 2080000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 260, "training_iteration": 26, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_14-24-01", "timestamp": 1704867841, "time_this_iter_s": 466.68537163734436, "time_total_s": 12318.209605932236, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x0000017129225DC0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 12318.209605932236, "timesteps_since_restore": 0, "iterations_since_restore": 26, "perf": {"cpu_util_percent": 17.679848484848485, "ram_util_percent": 85.65590909090908}}
{"episode_reward_max": -1200.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2216.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -300.0, "policy_1": -300.0, "policy_2": -300.0, "policy_3": -300.0}, "policy_reward_mean": {"policy_0": -554.0, "policy_1": -554.0, "policy_2": -554.0, "policy_3": -554.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-1600.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2000.0, -1600.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1200.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0], "policy_policy_1_reward": [-400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0], "policy_policy_2_reward": [-400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0], "policy_policy_3_reward": [-400.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6474956602417421, "mean_inference_ms": 5.094537259878181, "mean_action_processing_ms": 0.12776301619298625, "mean_env_wait_ms": 9.264597759845183, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 540000, "timesteps_this_iter": 0, "agent_timesteps_total": 2160000, "timers": {"sample_time_ms": 465997.235, "sample_throughput": 42.919, "load_time_ms": 1222.051, "load_throughput": 16365.925, "learn_time_ms": 167295.759, "learn_throughput": 119.549, "update_time_ms": 11.275}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 853.3396606445312, "cur_kl_coeff": 2.9802322387695314e-09, "cur_lr": 0.00037000000259999996, "total_loss": 2485.631005859375, "policy_loss": 5.25674813900423e-07, "vf_loss": 2485.631005859375, "vf_explained_var": 2.3245811462402344e-06, "kl": 3.211844227211935e-10, "entropy": 3.2188695907592773, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 722.0729370117188, "cur_kl_coeff": 2.9802322387695314e-09, "cur_lr": 0.00037000000259999996, "total_loss": 2479.497265625, "policy_loss": 4.312229276770552e-07, "vf_loss": 2479.497265625, "vf_explained_var": 2.849102020263672e-05, "kl": 4.722331609086083e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 769.2755737304688, "cur_kl_coeff": 2.9802322387695314e-09, "cur_lr": 0.00037000000259999996, "total_loss": 2472.953076171875, "policy_loss": 1.4922141966522418e-07, "vf_loss": 2472.953076171875, "vf_explained_var": 3.3736228942871094e-05, "kl": -5.761340182708353e-10, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 892.5145263671875, "cur_kl_coeff": 2.9802322387695314e-09, "cur_lr": 0.00037000000259999996, "total_loss": 2480.751220703125, "policy_loss": 3.496646925782443e-07, "vf_loss": 2480.751220703125, "vf_explained_var": -3.1113624572753906e-05, "kl": 8.485881247688454e-10, "entropy": 3.2188720703125, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 540000, "num_agent_steps_sampled": 2160000, "num_steps_trained": 540000, "num_agent_steps_trained": 2160000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 270, "training_iteration": 27, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_14-31-48", "timestamp": 1704868308, "time_this_iter_s": 466.714560508728, "time_total_s": 12784.924166440964, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170441CF040>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 12784.924166440964, "timesteps_since_restore": 0, "iterations_since_restore": 27, "perf": {"cpu_util_percent": 17.696827794561933, "ram_util_percent": 87.61148036253776}}
{"episode_reward_max": -1200.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2224.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -300.0, "policy_1": -300.0, "policy_2": -300.0, "policy_3": -300.0}, "policy_reward_mean": {"policy_0": -556.0, "policy_1": -556.0, "policy_2": -556.0, "policy_3": -556.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-1600.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2000.0, -1600.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1200.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0], "policy_policy_1_reward": [-400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0], "policy_policy_2_reward": [-400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0], "policy_policy_3_reward": [-400.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6467123899397222, "mean_inference_ms": 5.088030241651164, "mean_action_processing_ms": 0.12759934868412084, "mean_env_wait_ms": 9.258974975999251, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 560000, "timesteps_this_iter": 0, "agent_timesteps_total": 2240000, "timers": {"sample_time_ms": 466081.025, "sample_throughput": 42.911, "load_time_ms": 1220.715, "load_throughput": 16383.838, "learn_time_ms": 167393.157, "learn_throughput": 119.479, "update_time_ms": 11.502}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 852.6355590820312, "cur_kl_coeff": 1.4901161193847657e-09, "cur_lr": 0.0003650000027, "total_loss": 2219.1919921875, "policy_loss": 2.01702119717595e-07, "vf_loss": 2219.1919921875, "vf_explained_var": 1.0013580322265625e-05, "kl": 1.5519095897253977e-09, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 945.5032958984375, "cur_kl_coeff": 1.4901161193847657e-09, "cur_lr": 0.0003650000027, "total_loss": 2209.901806640625, "policy_loss": 2.4799346807213853e-07, "vf_loss": 2209.901806640625, "vf_explained_var": 2.7418136596679688e-06, "kl": 6.801502569486751e-10, "entropy": 3.218870687484741, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1011.5074462890625, "cur_kl_coeff": 1.4901161193847657e-09, "cur_lr": 0.0003650000027, "total_loss": 2213.231689453125, "policy_loss": 3.6068915916231957e-07, "vf_loss": 2213.231689453125, "vf_explained_var": 2.6881694793701172e-05, "kl": -5.556774856224633e-10, "entropy": 3.2188700675964355, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 905.2677001953125, "cur_kl_coeff": 1.4901161193847657e-09, "cur_lr": 0.0003650000027, "total_loss": 2234.739501953125, "policy_loss": 2.0864487328964288e-07, "vf_loss": 2234.739501953125, "vf_explained_var": -8.58306884765625e-06, "kl": 1.7178525546501787e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 560000, "num_agent_steps_sampled": 2240000, "num_steps_trained": 560000, "num_agent_steps_trained": 2240000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 280, "training_iteration": 28, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_14-39-36", "timestamp": 1704868776, "time_this_iter_s": 468.1066105365753, "time_total_s": 13253.030776977539, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170F9AB5F70>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 13253.030776977539, "timesteps_since_restore": 0, "iterations_since_restore": 28, "perf": {"cpu_util_percent": 17.71809954751131, "ram_util_percent": 88.44343891402715}}
{"episode_reward_max": -1200.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2252.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -300.0, "policy_1": -300.0, "policy_2": -300.0, "policy_3": -300.0}, "policy_reward_mean": {"policy_0": -563.0, "policy_1": -563.0, "policy_2": -563.0, "policy_3": -563.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1200.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0], "policy_policy_1_reward": [-600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0], "policy_policy_2_reward": [-600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0], "policy_policy_3_reward": [-600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6460532626798142, "mean_inference_ms": 5.082223459946585, "mean_action_processing_ms": 0.12746340199308043, "mean_env_wait_ms": 9.300132934365323, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 580000, "timesteps_this_iter": 0, "agent_timesteps_total": 2320000, "timers": {"sample_time_ms": 493141.1, "sample_throughput": 40.556, "load_time_ms": 1221.546, "load_throughput": 16372.691, "learn_time_ms": 167400.266, "learn_throughput": 119.474, "update_time_ms": 11.513}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1066.0235595703125, "cur_kl_coeff": 7.450580596923829e-10, "cur_lr": 0.0003600000028, "total_loss": 2476.604931640625, "policy_loss": 3.0081749813337666e-07, "vf_loss": 2476.604931640625, "vf_explained_var": 1.2516975402832031e-06, "kl": 1.1037097930044482e-09, "entropy": 3.2188690185546873, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 824.5263671875, "cur_kl_coeff": 7.450580596923829e-10, "cur_lr": 0.0003600000028, "total_loss": 2465.286962890625, "policy_loss": 3.2700539298602394e-07, "vf_loss": 2465.286962890625, "vf_explained_var": 1.9431114196777344e-05, "kl": -2.0682882351241716e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1002.9468994140625, "cur_kl_coeff": 7.450580596923829e-10, "cur_lr": 0.0003600000028, "total_loss": 2480.216259765625, "policy_loss": 3.541183411837068e-07, "vf_loss": 2480.216259765625, "vf_explained_var": 3.0338764190673828e-05, "kl": 5.327764028562054e-10, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 863.9197998046875, "cur_kl_coeff": 7.450580596923829e-10, "cur_lr": 0.0003600000028, "total_loss": 2478.88642578125, "policy_loss": 1.8033980975218356e-07, "vf_loss": 2478.88642578125, "vf_explained_var": -2.288818359375e-05, "kl": 2.159231965448427e-09, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 580000, "num_agent_steps_sampled": 2320000, "num_steps_trained": 580000, "num_agent_steps_trained": 2320000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 290, "training_iteration": 29, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_14-51-51", "timestamp": 1704869511, "time_this_iter_s": 734.8359289169312, "time_total_s": 13987.86670589447, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x0000017102DA3A60>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 13987.86670589447, "timesteps_since_restore": 0, "iterations_since_restore": 29, "perf": {"cpu_util_percent": 18.58288288288288, "ram_util_percent": 89.63093093093093}}
{"episode_reward_max": -1200.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2249.8, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -300.0, "policy_1": -300.0, "policy_2": -300.0, "policy_3": -300.0}, "policy_reward_mean": {"policy_0": -562.45, "policy_1": -562.45, "policy_2": -562.45, "policy_3": -562.45}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1200.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2180.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_1_reward": [-600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_2_reward": [-600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_3_reward": [-600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.645421640833879, "mean_inference_ms": 5.076928574635341, "mean_action_processing_ms": 0.12735326136470268, "mean_env_wait_ms": 9.340472060761108, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 600000, "timesteps_this_iter": 0, "agent_timesteps_total": 2400000, "timers": {"sample_time_ms": 493290.698, "sample_throughput": 40.544, "load_time_ms": 1224.33, "load_throughput": 16335.467, "learn_time_ms": 167512.309, "learn_throughput": 119.394, "update_time_ms": 11.48}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 922.2756958007812, "cur_kl_coeff": 3.7252902984619143e-10, "cur_lr": 0.00035500000289999997, "total_loss": 2466.22998046875, "policy_loss": 7.024765178442748e-08, "vf_loss": 2466.22998046875, "vf_explained_var": -6.079673767089844e-06, "kl": 8.078020101789929e-10, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1010.9268798828125, "cur_kl_coeff": 3.7252902984619143e-10, "cur_lr": 0.00035500000289999997, "total_loss": 2470.31552734375, "policy_loss": 1.9666672379514694e-07, "vf_loss": 2470.31552734375, "vf_explained_var": -8.344650268554688e-07, "kl": 3.8394318974122044e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1127.09423828125, "cur_kl_coeff": 3.7252902984619143e-10, "cur_lr": 0.00035500000289999997, "total_loss": 2471.821630859375, "policy_loss": 2.776145942240404e-07, "vf_loss": 2471.821630859375, "vf_explained_var": 2.8312206268310547e-05, "kl": 5.438680637792625e-10, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1114.194091796875, "cur_kl_coeff": 3.7252902984619143e-10, "cur_lr": 0.00035500000289999997, "total_loss": 2476.649853515625, "policy_loss": 2.314853613682999e-07, "vf_loss": 2476.649853515625, "vf_explained_var": -2.6106834411621094e-05, "kl": 3.339201207208653e-11, "entropy": 3.218871736526489, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 600000, "num_agent_steps_sampled": 2400000, "num_steps_trained": 600000, "num_agent_steps_trained": 2400000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 300, "training_iteration": 30, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_14-59-38", "timestamp": 1704869978, "time_this_iter_s": 467.06840658187866, "time_total_s": 14454.935112476349, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170441CF040>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 14454.935112476349, "timesteps_since_restore": 0, "iterations_since_restore": 30, "perf": {"cpu_util_percent": 19.059606656580936, "ram_util_percent": 80.72102874432677}}
{"episode_reward_max": -1200.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2245.8, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -300.0, "policy_1": -300.0, "policy_2": -300.0, "policy_3": -300.0}, "policy_reward_mean": {"policy_0": -561.45, "policy_1": -561.45, "policy_2": -561.45, "policy_3": -561.45}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1200.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2180.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0], "policy_policy_1_reward": [-600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0], "policy_policy_2_reward": [-600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0], "policy_policy_3_reward": [-600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6449290313868958, "mean_inference_ms": 5.071975727015173, "mean_action_processing_ms": 0.1272744641601598, "mean_env_wait_ms": 9.379576297258149, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 620000, "timesteps_this_iter": 0, "agent_timesteps_total": 2480000, "timers": {"sample_time_ms": 493320.016, "sample_throughput": 40.542, "load_time_ms": 1225.666, "load_throughput": 16317.654, "learn_time_ms": 167560.865, "learn_throughput": 119.36, "update_time_ms": 11.581}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1009.9417114257812, "cur_kl_coeff": 1.8626451492309571e-10, "cur_lr": 0.00035000000300000004, "total_loss": 2391.96552734375, "policy_loss": 1.8879890404388532e-07, "vf_loss": 2391.96552734375, "vf_explained_var": 2.682209014892578e-06, "kl": 4.1917351256692826e-10, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1483.4000244140625, "cur_kl_coeff": 1.8626451492309571e-10, "cur_lr": 0.00035000000300000004, "total_loss": 2400.70244140625, "policy_loss": 4.3091773918035867e-07, "vf_loss": 2400.70244140625, "vf_explained_var": 5.245208740234375e-06, "kl": 1.0764880209457317e-09, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1072.5731201171875, "cur_kl_coeff": 1.8626451492309571e-10, "cur_lr": 0.00035000000300000004, "total_loss": 2382.12587890625, "policy_loss": 3.2204626876364273e-07, "vf_loss": 2382.12587890625, "vf_explained_var": 2.7358531951904297e-05, "kl": 1.1122650828143676e-09, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1130.2530517578125, "cur_kl_coeff": 1.8626451492309571e-10, "cur_lr": 0.00035000000300000004, "total_loss": 2390.789501953125, "policy_loss": 2.6578903016272194e-07, "vf_loss": 2390.789501953125, "vf_explained_var": -6.9141387939453125e-06, "kl": 6.720888144629012e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 620000, "num_agent_steps_sampled": 2480000, "num_steps_trained": 620000, "num_agent_steps_trained": 2480000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 310, "training_iteration": 31, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_15-07-23", "timestamp": 1704870443, "time_this_iter_s": 465.179239988327, "time_total_s": 14920.114352464676, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A5700>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 14920.114352464676, "timesteps_since_restore": 0, "iterations_since_restore": 31, "perf": {"cpu_util_percent": 17.9154779969651, "ram_util_percent": 76.47132018209408}}
{"episode_reward_max": -1200.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2237.8, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -300.0, "policy_1": -300.0, "policy_2": -300.0, "policy_3": -300.0}, "policy_reward_mean": {"policy_0": -559.45, "policy_1": -559.45, "policy_2": -559.45, "policy_3": -559.45}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1200.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2180.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0], "policy_policy_1_reward": [-600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0], "policy_policy_2_reward": [-600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0], "policy_policy_3_reward": [-600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6446357101275721, "mean_inference_ms": 5.069157690777795, "mean_action_processing_ms": 0.1272235977421328, "mean_env_wait_ms": 9.419696693621194, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 640000, "timesteps_this_iter": 0, "agent_timesteps_total": 2560000, "timers": {"sample_time_ms": 495918.957, "sample_throughput": 40.329, "load_time_ms": 1231.624, "load_throughput": 16238.728, "learn_time_ms": 168812.337, "learn_throughput": 118.475, "update_time_ms": 11.778}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1110.517333984375, "cur_kl_coeff": 9.313225746154786e-11, "cur_lr": 0.0003450000031, "total_loss": 2273.737646484375, "policy_loss": 2.457714117554133e-07, "vf_loss": 2273.737646484375, "vf_explained_var": 1.3530254364013672e-05, "kl": -6.419013376390836e-11, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1106.350341796875, "cur_kl_coeff": 9.313225746154786e-11, "cur_lr": 0.0003450000031, "total_loss": 2274.2169921875, "policy_loss": 2.428722329028687e-07, "vf_loss": 2274.2169921875, "vf_explained_var": 8.940696716308594e-06, "kl": 6.406707886430496e-10, "entropy": 3.2188699722290037, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1068.374267578125, "cur_kl_coeff": 9.313225746154786e-11, "cur_lr": 0.0003450000031, "total_loss": 2281.005712890625, "policy_loss": 4.1067122670268573e-07, "vf_loss": 2281.005712890625, "vf_explained_var": 1.3947486877441406e-05, "kl": 1.7043357392054758e-09, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1116.018310546875, "cur_kl_coeff": 9.313225746154786e-11, "cur_lr": 0.0003450000031, "total_loss": 2286.410986328125, "policy_loss": 1.8363952501987414e-07, "vf_loss": 2286.410986328125, "vf_explained_var": -2.658367156982422e-05, "kl": 3.2717271669646665e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 640000, "num_agent_steps_sampled": 2560000, "num_steps_trained": 640000, "num_agent_steps_trained": 2560000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 320, "training_iteration": 32, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_15-15-47", "timestamp": 1704870947, "time_this_iter_s": 503.54867672920227, "time_total_s": 15423.663029193878, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A55E0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 15423.663029193878, "timesteps_since_restore": 0, "iterations_since_restore": 32, "perf": {"cpu_util_percent": 26.226863572433196, "ram_util_percent": 80.12728551336146}}
{"episode_reward_max": -1200.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2245.8, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -300.0, "policy_1": -300.0, "policy_2": -300.0, "policy_3": -300.0}, "policy_reward_mean": {"policy_0": -561.45, "policy_1": -561.45, "policy_2": -561.45, "policy_3": -561.45}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1200.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2180.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_1_reward": [-500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_2_reward": [-500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_3_reward": [-500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -300.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.644612355180367, "mean_inference_ms": 5.0682829590309035, "mean_action_processing_ms": 0.12722670133580993, "mean_env_wait_ms": 9.461056494734263, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 660000, "timesteps_this_iter": 0, "agent_timesteps_total": 2640000, "timers": {"sample_time_ms": 499934.552, "sample_throughput": 40.005, "load_time_ms": 1234.19, "load_throughput": 16204.963, "learn_time_ms": 169998.633, "learn_throughput": 117.648, "update_time_ms": 11.967}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1055.130126953125, "cur_kl_coeff": 4.656612873077393e-11, "cur_lr": 0.00034000000319999997, "total_loss": 2309.317529296875, "policy_loss": 2.1069526288108875e-07, "vf_loss": 2309.317529296875, "vf_explained_var": -2.7418136596679688e-06, "kl": -8.142852780945731e-10, "entropy": 3.2188690185546873, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 929.9495849609375, "cur_kl_coeff": 4.656612873077393e-11, "cur_lr": 0.00034000000319999997, "total_loss": 2298.3421875, "policy_loss": 3.9722442588541187e-07, "vf_loss": 2298.3421875, "vf_explained_var": 2.1457672119140625e-05, "kl": 2.135786710683041e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 919.3361206054688, "cur_kl_coeff": 4.656612873077393e-11, "cur_lr": 0.00034000000319999997, "total_loss": 2307.833935546875, "policy_loss": 1.7482757255216086e-07, "vf_loss": 2307.833935546875, "vf_explained_var": 4.208087921142578e-05, "kl": -8.316831001664582e-10, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 924.572265625, "cur_kl_coeff": 4.656612873077393e-11, "cur_lr": 0.00034000000319999997, "total_loss": 2299.287255859375, "policy_loss": 4.10051338484152e-07, "vf_loss": 2299.287255859375, "vf_explained_var": -1.9431114196777344e-05, "kl": 1.1960234830299398e-09, "entropy": 3.2188717842102053, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 660000, "num_agent_steps_sampled": 2640000, "num_steps_trained": 660000, "num_agent_steps_trained": 2640000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 330, "training_iteration": 33, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_15-24-11", "timestamp": 1704871451, "time_this_iter_s": 504.49774718284607, "time_total_s": 15928.160776376724, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170441CF040>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 15928.160776376724, "timesteps_since_restore": 0, "iterations_since_restore": 33, "perf": {"cpu_util_percent": 24.67955182072829, "ram_util_percent": 73.05560224089635}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2253.8, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -563.45, "policy_1": -563.45, "policy_2": -563.45, "policy_3": -563.45}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2180.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_1_reward": [-600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_2_reward": [-600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_3_reward": [-600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6447142684529945, "mean_inference_ms": 5.0693904339598275, "mean_action_processing_ms": 0.12728844639743428, "mean_env_wait_ms": 9.503435937893677, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 680000, "timesteps_this_iter": 0, "agent_timesteps_total": 2720000, "timers": {"sample_time_ms": 503940.933, "sample_throughput": 39.687, "load_time_ms": 1240.698, "load_throughput": 16119.96, "learn_time_ms": 171141.707, "learn_throughput": 116.862, "update_time_ms": 11.937}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1048.0894775390625, "cur_kl_coeff": 2.3283064365386964e-11, "cur_lr": 0.0003350000033, "total_loss": 2413.908251953125, "policy_loss": 3.887844068017898e-07, "vf_loss": 2413.908251953125, "vf_explained_var": 1.2040138244628906e-05, "kl": 1.1481819855996279e-09, "entropy": 3.218870162963867, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 967.7470703125, "cur_kl_coeff": 2.3283064365386964e-11, "cur_lr": 0.0003350000033, "total_loss": 2408.5181640625, "policy_loss": 4.1505813830333693e-07, "vf_loss": 2408.5181640625, "vf_explained_var": 4.410743713378906e-06, "kl": 6.10685106997022e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1000.3668212890625, "cur_kl_coeff": 2.3283064365386964e-11, "cur_lr": 0.0003350000033, "total_loss": 2418.344580078125, "policy_loss": 4.266548191078634e-07, "vf_loss": 2418.344580078125, "vf_explained_var": 2.2232532501220703e-05, "kl": 1.2555135528880685e-09, "entropy": 3.218869161605835, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1046.0279541015625, "cur_kl_coeff": 2.3283064365386964e-11, "cur_lr": 0.0003350000033, "total_loss": 2421.735302734375, "policy_loss": 8.864402571973073e-08, "vf_loss": 2421.735302734375, "vf_explained_var": -3.504753112792969e-05, "kl": 3.014559296632413e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 680000, "num_agent_steps_sampled": 2720000, "num_steps_trained": 680000, "num_agent_steps_trained": 2720000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 340, "training_iteration": 34, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_15-32-39", "timestamp": 1704871959, "time_this_iter_s": 507.44691371917725, "time_total_s": 16435.6076900959, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170441E0DC0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 16435.6076900959, "timesteps_since_restore": 0, "iterations_since_restore": 34, "perf": {"cpu_util_percent": 24.88147632311978, "ram_util_percent": 73.71671309192202}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2253.8, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -563.45, "policy_1": -563.45, "policy_2": -563.45, "policy_3": -563.45}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2180.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0], "policy_policy_1_reward": [-600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0], "policy_policy_2_reward": [-600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0], "policy_policy_3_reward": [-600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6450060363210358, "mean_inference_ms": 5.071905270438172, "mean_action_processing_ms": 0.1273672127061712, "mean_env_wait_ms": 9.546437777830485, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 700000, "timesteps_this_iter": 0, "agent_timesteps_total": 2800000, "timers": {"sample_time_ms": 507324.006, "sample_throughput": 39.423, "load_time_ms": 1246.878, "load_throughput": 16040.057, "learn_time_ms": 172766.569, "learn_throughput": 115.763, "update_time_ms": 11.935}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1198.2225341796875, "cur_kl_coeff": 1.1641532182693482e-11, "cur_lr": 0.0003300000034, "total_loss": 2293.834033203125, "policy_loss": 1.696300491182967e-07, "vf_loss": 2293.834033203125, "vf_explained_var": 2.205371856689453e-06, "kl": -1.2023149142947886e-11, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1112.1451416015625, "cur_kl_coeff": 1.1641532182693482e-11, "cur_lr": 0.0003300000034, "total_loss": 2278.641357421875, "policy_loss": 2.602767965154129e-07, "vf_loss": 2278.641357421875, "vf_explained_var": 1.4066696166992188e-05, "kl": 5.095381352959727e-10, "entropy": 3.218869924545288, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 937.6201171875, "cur_kl_coeff": 1.1641532182693482e-11, "cur_lr": 0.0003300000034, "total_loss": 2281.4951171875, "policy_loss": 3.145408575377928e-07, "vf_loss": 2281.4951171875, "vf_explained_var": 2.8371810913085938e-05, "kl": 2.434949913920903e-09, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1806.886474609375, "cur_kl_coeff": 1.1641532182693482e-11, "cur_lr": 0.0003300000034, "total_loss": 2302.796240234375, "policy_loss": 3.076076558805596e-07, "vf_loss": 2302.796240234375, "vf_explained_var": -1.049041748046875e-05, "kl": 2.926944873971049e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 700000, "num_agent_steps_sampled": 2800000, "num_steps_trained": 700000, "num_agent_steps_trained": 2800000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 350, "training_iteration": 35, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_15-41-04", "timestamp": 1704872464, "time_this_iter_s": 504.72529673576355, "time_total_s": 16940.332986831665, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170F9AB5F70>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 16940.332986831665, "timesteps_since_restore": 0, "iterations_since_restore": 35, "perf": {"cpu_util_percent": 24.28415147265077, "ram_util_percent": 73.69186535764376}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2253.8, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -563.45, "policy_1": -563.45, "policy_2": -563.45, "policy_3": -563.45}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2180.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0], "policy_policy_1_reward": [-500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0], "policy_policy_2_reward": [-500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0], "policy_policy_3_reward": [-500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6453408991922823, "mean_inference_ms": 5.0752222142636345, "mean_action_processing_ms": 0.1274566428439872, "mean_env_wait_ms": 9.58941194876851, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 720000, "timesteps_this_iter": 0, "agent_timesteps_total": 2880000, "timers": {"sample_time_ms": 510311.003, "sample_throughput": 39.192, "load_time_ms": 1251.84, "load_throughput": 15976.478, "learn_time_ms": 173451.678, "learn_throughput": 115.306, "update_time_ms": 12.234}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 922.7652587890625, "cur_kl_coeff": 5.820766091346741e-12, "cur_lr": 0.00032500000349999997, "total_loss": 2375.162353515625, "policy_loss": 3.8989067967509784e-07, "vf_loss": 2375.162353515625, "vf_explained_var": 2.3245811462402344e-06, "kl": -2.7848664763130416e-10, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 746.9241943359375, "cur_kl_coeff": 5.820766091346741e-12, "cur_lr": 0.00032500000349999997, "total_loss": 2350.2939453125, "policy_loss": 2.1471023132768606e-07, "vf_loss": 2350.2939453125, "vf_explained_var": 1.3709068298339844e-06, "kl": -8.080951963140848e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 888.4404296875, "cur_kl_coeff": 5.820766091346741e-12, "cur_lr": 0.00032500000349999997, "total_loss": 2366.931103515625, "policy_loss": 2.541446672221781e-07, "vf_loss": 2366.931103515625, "vf_explained_var": 2.2172927856445312e-05, "kl": -1.7195553037030465e-10, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 986.8312377929688, "cur_kl_coeff": 5.820766091346741e-12, "cur_lr": 0.00032500000349999997, "total_loss": 2380.948095703125, "policy_loss": 1.5105247137547905e-07, "vf_loss": 2380.948095703125, "vf_explained_var": -1.7523765563964844e-05, "kl": -4.171557849153018e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 720000, "num_agent_steps_sampled": 2880000, "num_steps_trained": 720000, "num_agent_steps_trained": 2880000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 360, "training_iteration": 36, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_15-49-11", "timestamp": 1704872951, "time_this_iter_s": 487.13767766952515, "time_total_s": 17427.47066450119, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A5430>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 17427.47066450119, "timesteps_since_restore": 0, "iterations_since_restore": 36, "perf": {"cpu_util_percent": 19.527576197387518, "ram_util_percent": 72.56226415094339}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2257.8, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -564.45, "policy_1": -564.45, "policy_2": -564.45, "policy_3": -564.45}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2180.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_1_reward": [-500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_2_reward": [-500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_3_reward": [-500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6458073688673323, "mean_inference_ms": 5.079272127191218, "mean_action_processing_ms": 0.1275613851597415, "mean_env_wait_ms": 9.632395306736726, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 740000, "timesteps_this_iter": 0, "agent_timesteps_total": 2960000, "timers": {"sample_time_ms": 512428.689, "sample_throughput": 39.03, "load_time_ms": 1257.402, "load_throughput": 15905.81, "learn_time_ms": 174788.784, "learn_throughput": 114.424, "update_time_ms": 12.618}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 912.03271484375, "cur_kl_coeff": 2.9103830456733705e-12, "cur_lr": 0.0003200000036, "total_loss": 2427.1857421875, "policy_loss": 3.748702969375017e-07, "vf_loss": 2427.1857421875, "vf_explained_var": 1.6689300537109375e-05, "kl": 1.2523819099830025e-09, "entropy": 3.218870162963867, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1061.673828125, "cur_kl_coeff": 2.9103830456733705e-12, "cur_lr": 0.0003200000036, "total_loss": 2422.391748046875, "policy_loss": 3.1306266778940994e-07, "vf_loss": 2422.391748046875, "vf_explained_var": 4.827976226806641e-06, "kl": 2.9104791354761515e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 943.9308471679688, "cur_kl_coeff": 2.9103830456733705e-12, "cur_lr": 0.0003200000036, "total_loss": 2428.30517578125, "policy_loss": 2.481651312002953e-07, "vf_loss": 2428.30517578125, "vf_explained_var": 2.7060508728027344e-05, "kl": -1.9258857009152308e-10, "entropy": 3.218868923187256, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1023.7742919921875, "cur_kl_coeff": 2.9103830456733705e-12, "cur_lr": 0.0003200000036, "total_loss": 2442.90908203125, "policy_loss": 2.9761314443987887e-07, "vf_loss": 2442.90908203125, "vf_explained_var": -1.7881393432617188e-05, "kl": 8.064238964100579e-11, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 740000, "num_agent_steps_sampled": 2960000, "num_steps_trained": 740000, "num_agent_steps_trained": 2960000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 370, "training_iteration": 37, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_15-57-25", "timestamp": 1704873445, "time_this_iter_s": 494.4043915271759, "time_total_s": 17921.875056028366, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170FC8ED310>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 17921.875056028366, "timesteps_since_restore": 0, "iterations_since_restore": 37, "perf": {"cpu_util_percent": 22.010443490701004, "ram_util_percent": 74.60872675250357}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2257.8, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -564.45, "policy_1": -564.45, "policy_2": -564.45, "policy_3": -564.45}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2180.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0], "policy_policy_1_reward": [-600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0], "policy_policy_2_reward": [-600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0], "policy_policy_3_reward": [-600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6465298458110429, "mean_inference_ms": 5.084774353983612, "mean_action_processing_ms": 0.12769040703930606, "mean_env_wait_ms": 9.675990518978375, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 760000, "timesteps_this_iter": 0, "agent_timesteps_total": 3040000, "timers": {"sample_time_ms": 516397.512, "sample_throughput": 38.73, "load_time_ms": 1257.965, "load_throughput": 15898.694, "learn_time_ms": 174761.244, "learn_throughput": 114.442, "update_time_ms": 12.615}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1132.87744140625, "cur_kl_coeff": 1.4551915228366853e-12, "cur_lr": 0.0003150000037, "total_loss": 2361.443359375, "policy_loss": 2.2342681259557297e-07, "vf_loss": 2361.443359375, "vf_explained_var": -2.7418136596679688e-06, "kl": 3.7981332350056627e-10, "entropy": 3.2188690185546873, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1102.4822998046875, "cur_kl_coeff": 1.4551915228366853e-12, "cur_lr": 0.0003150000037, "total_loss": 2355.027490234375, "policy_loss": 3.337860123853176e-07, "vf_loss": 2355.027490234375, "vf_explained_var": -8.821487426757812e-06, "kl": -7.175277733084684e-10, "entropy": 3.218870258331299, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1217.5478515625, "cur_kl_coeff": 1.4551915228366853e-12, "cur_lr": 0.0003150000037, "total_loss": 2363.70185546875, "policy_loss": 4.955673229112279e-07, "vf_loss": 2363.70185546875, "vf_explained_var": 2.86102294921875e-05, "kl": 2.257980920294944e-09, "entropy": 3.2188690185546873, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1081.510498046875, "cur_kl_coeff": 1.4551915228366853e-12, "cur_lr": 0.0003150000037, "total_loss": 2363.945654296875, "policy_loss": 3.909873892204985e-07, "vf_loss": 2363.945654296875, "vf_explained_var": -2.0503997802734375e-05, "kl": 9.416414759044756e-10, "entropy": 3.218871593475342, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 760000, "num_agent_steps_sampled": 3040000, "num_steps_trained": 760000, "num_agent_steps_trained": 3040000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 380, "training_iteration": 38, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_16-05-39", "timestamp": 1704873939, "time_this_iter_s": 494.10860109329224, "time_total_s": 18415.98365712166, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170F9AB5F70>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 18415.98365712166, "timesteps_since_restore": 0, "iterations_since_restore": 38, "perf": {"cpu_util_percent": 21.889556509299002, "ram_util_percent": 76.70271816881258}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2245.8, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -561.45, "policy_1": -561.45, "policy_2": -561.45, "policy_3": -561.45}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2400.0, -2180.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0], "policy_policy_1_reward": [-500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0], "policy_policy_2_reward": [-500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0], "policy_policy_3_reward": [-500.0, -600.0, -600.0, -545.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6472645916603318, "mean_inference_ms": 5.090413269698347, "mean_action_processing_ms": 0.12783225758716607, "mean_env_wait_ms": 9.673039082655215, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 780000, "timesteps_this_iter": 0, "agent_timesteps_total": 3120000, "timers": {"sample_time_ms": 490263.044, "sample_throughput": 40.794, "load_time_ms": 1255.83, "load_throughput": 15925.724, "learn_time_ms": 175291.195, "learn_throughput": 114.096, "update_time_ms": 13.02}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 872.0357666015625, "cur_kl_coeff": 7.275957614183426e-13, "cur_lr": 0.0003100000038, "total_loss": 2080.902783203125, "policy_loss": 3.025436404513471e-07, "vf_loss": 2080.902783203125, "vf_explained_var": -7.62939453125e-06, "kl": 3.6588004154181064e-10, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 772.8372802734375, "cur_kl_coeff": 7.275957614183426e-13, "cur_lr": 0.0003100000038, "total_loss": 2082.09775390625, "policy_loss": 9.272575063334898e-08, "vf_loss": 2082.09775390625, "vf_explained_var": -1.3828277587890625e-05, "kl": 6.059117096945954e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 969.9510498046875, "cur_kl_coeff": 7.275957614183426e-13, "cur_lr": 0.0003100000038, "total_loss": 2099.087109375, "policy_loss": 2.7082444014336906e-07, "vf_loss": 2099.087109375, "vf_explained_var": 2.3543834686279297e-05, "kl": 7.541862523385845e-10, "entropy": 3.2188693046569825, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1109.4527587890625, "cur_kl_coeff": 7.275957614183426e-13, "cur_lr": 0.0003100000038, "total_loss": 2097.5080078125, "policy_loss": 2.379703524013621e-07, "vf_loss": 2097.5080078125, "vf_explained_var": -3.731250762939453e-05, "kl": 2.606403091964804e-09, "entropy": 3.218871736526489, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 780000, "num_agent_steps_sampled": 3120000, "num_steps_trained": 780000, "num_agent_steps_trained": 3120000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 390, "training_iteration": 39, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_16-13-39", "timestamp": 1704874419, "time_this_iter_s": 479.27482867240906, "time_total_s": 18895.258485794067, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A5430>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 18895.258485794067, "timesteps_since_restore": 0, "iterations_since_restore": 39, "perf": {"cpu_util_percent": 19.191445427728613, "ram_util_percent": 75.59867256637169}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2236.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -559.0, "policy_1": -559.0, "policy_2": -559.0, "policy_3": -559.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0], "policy_policy_1_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0], "policy_policy_2_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0], "policy_policy_3_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6481023000510518, "mean_inference_ms": 5.0968503085481025, "mean_action_processing_ms": 0.1279852402647193, "mean_env_wait_ms": 9.672056599952105, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 800000, "timesteps_this_iter": 0, "agent_timesteps_total": 3200000, "timers": {"sample_time_ms": 492702.866, "sample_throughput": 40.592, "load_time_ms": 1259.14, "load_throughput": 15883.855, "learn_time_ms": 176069.98, "learn_throughput": 113.591, "update_time_ms": 13.022}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1063.75830078125, "cur_kl_coeff": 3.637978807091713e-13, "cur_lr": 0.0003050000039, "total_loss": 2252.90087890625, "policy_loss": 3.0424118357963437e-07, "vf_loss": 2252.90087890625, "vf_explained_var": 6.616115570068359e-06, "kl": 1.6104223510282623e-10, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1046.1610107421875, "cur_kl_coeff": 3.637978807091713e-13, "cur_lr": 0.0003050000039, "total_loss": 2234.066796875, "policy_loss": 2.4291038882573444e-07, "vf_loss": 2234.066796875, "vf_explained_var": 2.777576446533203e-05, "kl": 3.7086445997536187e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1065.0982666015625, "cur_kl_coeff": 3.637978807091713e-13, "cur_lr": 0.0003050000039, "total_loss": 2254.889990234375, "policy_loss": 3.170299600174786e-07, "vf_loss": 2254.889990234375, "vf_explained_var": 3.349781036376953e-05, "kl": 1.3661570194556384e-09, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1110.3485107421875, "cur_kl_coeff": 3.637978807091713e-13, "cur_lr": 0.0003050000039, "total_loss": 2246.964794921875, "policy_loss": 2.0570754137583247e-07, "vf_loss": 2246.964794921875, "vf_explained_var": -2.3126602172851562e-05, "kl": -2.92710367055804e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 800000, "num_agent_steps_sampled": 3200000, "num_steps_trained": 800000, "num_agent_steps_trained": 3200000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 400, "training_iteration": 40, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_16-21-53", "timestamp": 1704874913, "time_this_iter_s": 493.97175121307373, "time_total_s": 19389.23023700714, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x0000017102DA3CA0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 19389.23023700714, "timesteps_since_restore": 0, "iterations_since_restore": 40, "perf": {"cpu_util_percent": 21.73218884120172, "ram_util_percent": 79.75979971387696}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2232.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -558.0, "policy_1": -558.0, "policy_2": -558.0, "policy_3": -558.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0], "policy_policy_1_reward": [-600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0], "policy_policy_2_reward": [-600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0], "policy_policy_3_reward": [-600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -400.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6489797801126242, "mean_inference_ms": 5.103615555068841, "mean_action_processing_ms": 0.1281365075124876, "mean_env_wait_ms": 9.671908842529042, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 820000, "timesteps_this_iter": 0, "agent_timesteps_total": 3280000, "timers": {"sample_time_ms": 494144.302, "sample_throughput": 40.474, "load_time_ms": 1258.135, "load_throughput": 15896.55, "learn_time_ms": 176227.735, "learn_throughput": 113.49, "update_time_ms": 13.001}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1128.762939453125, "cur_kl_coeff": 1.8189894035458566e-13, "cur_lr": 0.000300000004, "total_loss": 2323.4828125, "policy_loss": 3.083133691017537e-07, "vf_loss": 2323.4828125, "vf_explained_var": 1.9729137420654297e-05, "kl": 1.901426016637231e-10, "entropy": 3.2188692569732664, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1107.0595703125, "cur_kl_coeff": 1.8189894035458566e-13, "cur_lr": 0.000300000004, "total_loss": 2310.3267578125, "policy_loss": 1.8897056466293805e-07, "vf_loss": 2310.3267578125, "vf_explained_var": 1.0013580322265625e-05, "kl": -1.0608630900826865e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1204.677490234375, "cur_kl_coeff": 1.8189894035458566e-13, "cur_lr": 0.000300000004, "total_loss": 2330.970556640625, "policy_loss": 3.158092588773087e-07, "vf_loss": 2330.970556640625, "vf_explained_var": 3.0994415283203125e-05, "kl": 1.5937409586497254e-10, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1000.0091552734375, "cur_kl_coeff": 1.8189894035458566e-13, "cur_lr": 0.000300000004, "total_loss": 2333.27607421875, "policy_loss": 3.0073165646626876e-07, "vf_loss": 2333.27607421875, "vf_explained_var": -1.0132789611816406e-05, "kl": 1.5537203246207555e-09, "entropy": 3.218871593475342, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 820000, "num_agent_steps_sampled": 3280000, "num_steps_trained": 820000, "num_agent_steps_trained": 3280000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 410, "training_iteration": 41, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_16-29-46", "timestamp": 1704875386, "time_this_iter_s": 473.33866786956787, "time_total_s": 19862.56890487671, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A53A0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 19862.56890487671, "timesteps_since_restore": 0, "iterations_since_restore": 41, "perf": {"cpu_util_percent": 18.741405082212257, "ram_util_percent": 77.84140508221225}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2224.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -556.0, "policy_1": -556.0, "policy_2": -556.0, "policy_3": -556.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0], "policy_policy_1_reward": [-600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0], "policy_policy_2_reward": [-600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0], "policy_policy_3_reward": [-600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6497055106771132, "mean_inference_ms": 5.108806958899017, "mean_action_processing_ms": 0.12826329135967726, "mean_env_wait_ms": 9.670657165868489, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 840000, "timesteps_this_iter": 0, "agent_timesteps_total": 3360000, "timers": {"sample_time_ms": 492435.632, "sample_throughput": 40.614, "load_time_ms": 1253.672, "load_throughput": 15953.135, "learn_time_ms": 175207.107, "learn_throughput": 114.151, "update_time_ms": 12.702}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 862.1168823242188, "cur_kl_coeff": 9.094947017729283e-14, "cur_lr": 0.0002950000041, "total_loss": 2311.987890625, "policy_loss": 3.2089233683585403e-07, "vf_loss": 2311.987890625, "vf_explained_var": -9.298324584960938e-06, "kl": -3.820085889977287e-10, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 863.8766479492188, "cur_kl_coeff": 9.094947017729283e-14, "cur_lr": 0.0002950000041, "total_loss": 2309.0443359375, "policy_loss": 2.385425562079213e-07, "vf_loss": 2309.0443359375, "vf_explained_var": 1.4901161193847656e-06, "kl": -6.777485350717427e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 726.7083129882812, "cur_kl_coeff": 9.094947017729283e-14, "cur_lr": 0.0002950000041, "total_loss": 2315.311572265625, "policy_loss": 3.8543700222604114e-07, "vf_loss": 2315.311572265625, "vf_explained_var": 3.2901763916015625e-05, "kl": 5.932706374778273e-11, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 911.3767700195312, "cur_kl_coeff": 9.094947017729283e-14, "cur_lr": 0.0002950000041, "total_loss": 2321.205224609375, "policy_loss": 3.4941673145993943e-07, "vf_loss": 2321.205224609375, "vf_explained_var": -1.4424324035644531e-05, "kl": -3.0987205301702405e-10, "entropy": 3.218871593475342, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 840000, "num_agent_steps_sampled": 3360000, "num_steps_trained": 840000, "num_agent_steps_trained": 3360000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 420, "training_iteration": 42, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_16-37-41", "timestamp": 1704875861, "time_this_iter_s": 474.6204493045807, "time_total_s": 20337.18935418129, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x0000017044293310>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 20337.18935418129, "timesteps_since_restore": 0, "iterations_since_restore": 42, "perf": {"cpu_util_percent": 18.504023845007453, "ram_util_percent": 78.43755588673622}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2216.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -554.0, "policy_1": -554.0, "policy_2": -554.0, "policy_3": -554.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0], "policy_policy_1_reward": [-600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0], "policy_policy_2_reward": [-600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0], "policy_policy_3_reward": [-600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6503749463639354, "mean_inference_ms": 5.113457018013928, "mean_action_processing_ms": 0.12837259756361535, "mean_env_wait_ms": 9.669066769480896, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 860000, "timesteps_this_iter": 0, "agent_timesteps_total": 3440000, "timers": {"sample_time_ms": 491072.268, "sample_throughput": 40.727, "load_time_ms": 1254.977, "load_throughput": 15936.551, "learn_time_ms": 174442.786, "learn_throughput": 114.651, "update_time_ms": 12.799}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1714.262939453125, "cur_kl_coeff": 4.5474735088646414e-14, "cur_lr": 0.0002900000042, "total_loss": 2106.734765625, "policy_loss": 3.3568382331594646e-07, "vf_loss": 2106.734765625, "vf_explained_var": 1.049041748046875e-05, "kl": 6.426206927701017e-10, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1517.947509765625, "cur_kl_coeff": 4.5474735088646414e-14, "cur_lr": 0.0002900000042, "total_loss": 2090.316259765625, "policy_loss": 3.191375711253386e-07, "vf_loss": 2090.316259765625, "vf_explained_var": 9.655952453613281e-06, "kl": 8.043586754991061e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1933.614013671875, "cur_kl_coeff": 4.5474735088646414e-14, "cur_lr": 0.0002900000042, "total_loss": 2112.6896484375, "policy_loss": 4.1629790619168717e-07, "vf_loss": 2112.6896484375, "vf_explained_var": 2.384185791015625e-05, "kl": 1.3888535460582219e-09, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 2361.77294921875, "cur_kl_coeff": 4.5474735088646414e-14, "cur_lr": 0.0002900000042, "total_loss": 2129.841064453125, "policy_loss": 2.3335457255413417e-07, "vf_loss": 2129.841064453125, "vf_explained_var": -4.208087921142578e-05, "kl": -7.7493612568591e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 860000, "num_agent_steps_sampled": 3440000, "num_steps_trained": 860000, "num_agent_steps_trained": 3440000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 430, "training_iteration": 43, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_16-45-54", "timestamp": 1704876354, "time_this_iter_s": 493.50794291496277, "time_total_s": 20830.697297096252, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x0000017102DA3DC0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 20830.697297096252, "timesteps_since_restore": 0, "iterations_since_restore": 43, "perf": {"cpu_util_percent": 21.595988538681947, "ram_util_percent": 83.11848137535817}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2200.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -550.0, "policy_1": -550.0, "policy_2": -550.0, "policy_3": -550.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -1600.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0], "policy_policy_1_reward": [-500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0], "policy_policy_2_reward": [-500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0], "policy_policy_3_reward": [-500.0, -600.0, -400.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6510184460870119, "mean_inference_ms": 5.11749145849209, "mean_action_processing_ms": 0.12845058291539999, "mean_env_wait_ms": 9.666974418664616, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 880000, "timesteps_this_iter": 0, "agent_timesteps_total": 3520000, "timers": {"sample_time_ms": 489812.969, "sample_throughput": 40.832, "load_time_ms": 1258.629, "load_throughput": 15890.303, "learn_time_ms": 174089.768, "learn_throughput": 114.883, "update_time_ms": 12.799}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 996.6103515625, "cur_kl_coeff": 2.2737367544323207e-14, "cur_lr": 0.0002850000043, "total_loss": 1954.76279296875, "policy_loss": 3.7206650622678693e-07, "vf_loss": 1954.76279296875, "vf_explained_var": -1.800060272216797e-05, "kl": -6.373161443029574e-10, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1167.6617431640625, "cur_kl_coeff": 2.2737367544323207e-14, "cur_lr": 0.0002850000043, "total_loss": 1953.4096435546876, "policy_loss": 4.761791140950322e-07, "vf_loss": 1953.4096435546876, "vf_explained_var": 1.0728836059570312e-05, "kl": 6.663745111268327e-11, "entropy": 3.218871593475342, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 917.65283203125, "cur_kl_coeff": 2.2737367544323207e-14, "cur_lr": 0.0002850000043, "total_loss": 1945.487548828125, "policy_loss": 1.9626617326906625e-07, "vf_loss": 1945.487548828125, "vf_explained_var": 3.516674041748047e-05, "kl": -8.001952531144774e-10, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1007.21875, "cur_kl_coeff": 2.2737367544323207e-14, "cur_lr": 0.0002850000043, "total_loss": 1954.6044677734376, "policy_loss": 2.791309341221293e-07, "vf_loss": 1954.6044677734376, "vf_explained_var": -1.049041748046875e-05, "kl": 5.852911509618419e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 880000, "num_agent_steps_sampled": 3520000, "num_steps_trained": 880000, "num_agent_steps_trained": 3520000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 440, "training_iteration": 44, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_16-54-13", "timestamp": 1704876853, "time_this_iter_s": 499.0118463039398, "time_total_s": 21329.709143400192, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170442933A0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 21329.709143400192, "timesteps_since_restore": 0, "iterations_since_restore": 44, "perf": {"cpu_util_percent": 22.858781869688386, "ram_util_percent": 83.56558073654391}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2220.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -555.0, "policy_1": -555.0, "policy_2": -555.0, "policy_3": -555.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_1_reward": [-600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_2_reward": [-600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_3_reward": [-600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.651594982364569, "mean_inference_ms": 5.121032289858443, "mean_action_processing_ms": 0.12853102542740089, "mean_env_wait_ms": 9.6646793859822, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 900000, "timesteps_this_iter": 0, "agent_timesteps_total": 3600000, "timers": {"sample_time_ms": 489035.394, "sample_throughput": 40.897, "load_time_ms": 1258.989, "load_throughput": 15885.759, "learn_time_ms": 172994.741, "learn_throughput": 115.61, "update_time_ms": 12.702}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 978.7841796875, "cur_kl_coeff": 1.1368683772161604e-14, "cur_lr": 0.0002800000044, "total_loss": 2563.58330078125, "policy_loss": 3.0464171620891987e-07, "vf_loss": 2563.58330078125, "vf_explained_var": 1.0907649993896484e-05, "kl": 5.791609594685276e-10, "entropy": 3.218869924545288, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1235.417236328125, "cur_kl_coeff": 1.1368683772161604e-14, "cur_lr": 0.0002800000044, "total_loss": 2582.272607421875, "policy_loss": 2.8666496438845e-07, "vf_loss": 2582.272607421875, "vf_explained_var": 2.8014183044433594e-06, "kl": 1.427865789493943e-09, "entropy": 3.21887001991272, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1131.35546875, "cur_kl_coeff": 1.1368683772161604e-14, "cur_lr": 0.0002800000044, "total_loss": 2584.011279296875, "policy_loss": 2.1107673414677564e-07, "vf_loss": 2584.011279296875, "vf_explained_var": 2.777576446533203e-05, "kl": 3.466892550818557e-10, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1205.357421875, "cur_kl_coeff": 1.1368683772161604e-14, "cur_lr": 0.0002800000044, "total_loss": 2575.043701171875, "policy_loss": 4.3447494619819337e-07, "vf_loss": 2575.043701171875, "vf_explained_var": -2.2292137145996094e-05, "kl": 3.534421387507702e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 900000, "num_agent_steps_sampled": 3600000, "num_steps_trained": 900000, "num_agent_steps_trained": 3600000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 450, "training_iteration": 45, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_17-02-23", "timestamp": 1704877343, "time_this_iter_s": 489.48524594306946, "time_total_s": 21819.19438934326, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x0000017044293310>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 21819.19438934326, "timesteps_since_restore": 0, "iterations_since_restore": 45, "perf": {"cpu_util_percent": 20.870274170274172, "ram_util_percent": 84.64069264069263}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2212.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -553.0, "policy_1": -553.0, "policy_2": -553.0, "policy_3": -553.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2400.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0], "policy_policy_1_reward": [-600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0], "policy_policy_2_reward": [-600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0], "policy_policy_3_reward": [-600.0, -600.0, -500.0, -600.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6520564752250527, "mean_inference_ms": 5.123996024354088, "mean_action_processing_ms": 0.1286030486763548, "mean_env_wait_ms": 9.661856791846931, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 920000, "timesteps_this_iter": 0, "agent_timesteps_total": 3680000, "timers": {"sample_time_ms": 486842.488, "sample_throughput": 41.081, "load_time_ms": 1254.291, "load_throughput": 15945.261, "learn_time_ms": 172450.995, "learn_throughput": 115.975, "update_time_ms": 12.404}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 855.4196166992188, "cur_kl_coeff": 5.684341886080802e-15, "cur_lr": 0.00027500000449999994, "total_loss": 2224.395849609375, "policy_loss": 4.0977477020476274e-07, "vf_loss": 2224.395849609375, "vf_explained_var": -1.0967254638671875e-05, "kl": 1.2273115808369984e-09, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1051.798583984375, "cur_kl_coeff": 5.684341886080802e-15, "cur_lr": 0.00027500000449999994, "total_loss": 2213.88330078125, "policy_loss": 1.9265175419391766e-07, "vf_loss": 2213.88330078125, "vf_explained_var": 9.655952453613281e-06, "kl": -6.92045762362703e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 947.6740112304688, "cur_kl_coeff": 5.684341886080802e-15, "cur_lr": 0.00027500000449999994, "total_loss": 2225.26513671875, "policy_loss": 1.1734008955599507e-07, "vf_loss": 2225.26513671875, "vf_explained_var": 5.8710575103759766e-05, "kl": 7.092924858564942e-10, "entropy": 3.2188683986663817, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 970.1727294921875, "cur_kl_coeff": 5.684341886080802e-15, "cur_lr": 0.00027500000449999994, "total_loss": 2228.76396484375, "policy_loss": 2.714920111612784e-07, "vf_loss": 2228.76396484375, "vf_explained_var": -1.3113021850585938e-05, "kl": 1.7254207812511524e-09, "entropy": 3.218871593475342, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 920000, "num_agent_steps_sampled": 3680000, "num_steps_trained": 920000, "num_agent_steps_trained": 3680000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 460, "training_iteration": 46, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_17-10-14", "timestamp": 1704877814, "time_this_iter_s": 470.6783878803253, "time_total_s": 22289.872777223587, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A5AF0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 22289.872777223587, "timesteps_since_restore": 0, "iterations_since_restore": 46, "perf": {"cpu_util_percent": 18.209595202398802, "ram_util_percent": 83.26446776611695}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2204.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -551.0, "policy_1": -551.0, "policy_2": -551.0, "policy_3": -551.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0], "policy_policy_1_reward": [-500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0], "policy_policy_2_reward": [-500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0], "policy_policy_3_reward": [-500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6526264814440907, "mean_inference_ms": 5.128675862543122, "mean_action_processing_ms": 0.12870389017774486, "mean_env_wait_ms": 9.661331710797743, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 940000, "timesteps_this_iter": 0, "agent_timesteps_total": 3760000, "timers": {"sample_time_ms": 490227.109, "sample_throughput": 40.797, "load_time_ms": 1276.557, "load_throughput": 15667.14, "learn_time_ms": 188119.956, "learn_throughput": 106.315, "update_time_ms": 12.307}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1887.5113525390625, "cur_kl_coeff": 2.842170943040401e-15, "cur_lr": 0.0002700000046, "total_loss": 2148.68876953125, "policy_loss": 1.4606475851053347e-07, "vf_loss": 2148.68876953125, "vf_explained_var": 2.2351741790771484e-05, "kl": -1.7906389149224822e-09, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1479.057373046875, "cur_kl_coeff": 2.842170943040401e-15, "cur_lr": 0.0002700000046, "total_loss": 2135.239208984375, "policy_loss": 3.8469314769251637e-07, "vf_loss": 2135.239208984375, "vf_explained_var": 2.282857894897461e-05, "kl": -7.660493434036298e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1837.5130615234375, "cur_kl_coeff": 2.842170943040401e-15, "cur_lr": 0.0002700000046, "total_loss": 2138.4419921875, "policy_loss": 1.2260436759170546e-07, "vf_loss": 2138.4419921875, "vf_explained_var": 1.4960765838623047e-05, "kl": -1.0146205743633008e-09, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1865.693115234375, "cur_kl_coeff": 2.842170943040401e-15, "cur_lr": 0.0002700000046, "total_loss": 2185.12548828125, "policy_loss": 1.7886161352009822e-07, "vf_loss": 2185.12548828125, "vf_explained_var": -9.179115295410156e-06, "kl": 3.2908982761092885e-10, "entropy": 3.218871593475342, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 940000, "num_agent_steps_sampled": 3760000, "num_steps_trained": 940000, "num_agent_steps_trained": 3760000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 470, "training_iteration": 47, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_17-21-44", "timestamp": 1704878504, "time_this_iter_s": 690.6583170890808, "time_total_s": 22980.531094312668, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A51F0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 22980.531094312668, "timesteps_since_restore": 0, "iterations_since_restore": 47, "perf": {"cpu_util_percent": 47.81294964028777, "ram_util_percent": 87.0065775950668}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2196.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -549.0, "policy_1": -549.0, "policy_2": -549.0, "policy_3": -549.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -1600.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0], "policy_policy_1_reward": [-500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0], "policy_policy_2_reward": [-500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0], "policy_policy_3_reward": [-500.0, -400.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6533462414932124, "mean_inference_ms": 5.13519213317556, "mean_action_processing_ms": 0.1288327649347671, "mean_env_wait_ms": 9.663278385635108, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 960000, "timesteps_this_iter": 0, "agent_timesteps_total": 3840000, "timers": {"sample_time_ms": 510696.279, "sample_throughput": 39.162, "load_time_ms": 1323.204, "load_throughput": 15114.823, "learn_time_ms": 191511.525, "learn_throughput": 104.432, "update_time_ms": 12.18}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 990.5162963867188, "cur_kl_coeff": 1.4210854715202005e-15, "cur_lr": 0.0002650000047, "total_loss": 2118.31962890625, "policy_loss": 3.3371925518554945e-07, "vf_loss": 2118.31962890625, "vf_explained_var": 9.000301361083984e-06, "kl": 5.256285184884746e-10, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1047.064453125, "cur_kl_coeff": 1.4210854715202005e-15, "cur_lr": 0.0002650000047, "total_loss": 2114.116064453125, "policy_loss": 3.147602179787157e-07, "vf_loss": 2114.116064453125, "vf_explained_var": 1.6987323760986328e-05, "kl": 4.4258229303650863e-10, "entropy": 3.218870210647583, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1090.727294921875, "cur_kl_coeff": 1.4210854715202005e-15, "cur_lr": 0.0002650000047, "total_loss": 2117.780908203125, "policy_loss": 2.408123019215225e-07, "vf_loss": 2117.780908203125, "vf_explained_var": 2.956390380859375e-05, "kl": -1.400288121566895e-10, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 913.2545166015625, "cur_kl_coeff": 1.4210854715202005e-15, "cur_lr": 0.0002650000047, "total_loss": 2114.998779296875, "policy_loss": 3.523158986773467e-07, "vf_loss": 2114.998779296875, "vf_explained_var": -1.5616416931152344e-05, "kl": 8.645505866766712e-10, "entropy": 3.2188717842102053, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 960000, "num_agent_steps_sampled": 3840000, "num_steps_trained": 960000, "num_agent_steps_trained": 3840000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 480, "training_iteration": 48, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_17-31-21", "timestamp": 1704879081, "time_this_iter_s": 576.2670030593872, "time_total_s": 23556.798097372055, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A53A0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 23556.798097372055, "timesteps_since_restore": 0, "iterations_since_restore": 48, "perf": {"cpu_util_percent": 33.27429274292743, "ram_util_percent": 85.8392373923739}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2204.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -551.0, "policy_1": -551.0, "policy_2": -551.0, "policy_3": -551.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0], "policy_policy_1_reward": [-600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0], "policy_policy_2_reward": [-600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0], "policy_policy_3_reward": [-600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6540344678180958, "mean_inference_ms": 5.141461818974901, "mean_action_processing_ms": 0.12894975630143474, "mean_env_wait_ms": 9.665193952849313, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 980000, "timesteps_this_iter": 0, "agent_timesteps_total": 3920000, "timers": {"sample_time_ms": 513825.736, "sample_throughput": 38.924, "load_time_ms": 1319.608, "load_throughput": 15156.016, "learn_time_ms": 190968.171, "learn_throughput": 104.729, "update_time_ms": 11.978}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1369.3922119140625, "cur_kl_coeff": 7.105427357601002e-16, "cur_lr": 0.0002600000048, "total_loss": 2413.02998046875, "policy_loss": 4.573821975606407e-07, "vf_loss": 2413.02998046875, "vf_explained_var": 8.046627044677734e-06, "kl": 3.5689201594024667e-11, "entropy": 3.2188692569732664, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1055.406982421875, "cur_kl_coeff": 7.105427357601002e-16, "cur_lr": 0.0002600000048, "total_loss": 2400.774755859375, "policy_loss": 3.211116814227921e-07, "vf_loss": 2400.774755859375, "vf_explained_var": 6.556510925292969e-06, "kl": 1.6780958261852774e-09, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1186.5838623046875, "cur_kl_coeff": 7.105427357601002e-16, "cur_lr": 0.0002600000048, "total_loss": 2420.310546875, "policy_loss": 2.3699760615114585e-07, "vf_loss": 2420.310546875, "vf_explained_var": 1.71661376953125e-05, "kl": 1.762689860740352e-09, "entropy": 3.2188684940338135, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1236.0074462890625, "cur_kl_coeff": 7.105427357601002e-16, "cur_lr": 0.0002600000048, "total_loss": 2422.866650390625, "policy_loss": 2.3090362075706138e-07, "vf_loss": 2422.866650390625, "vf_explained_var": -2.3245811462402344e-05, "kl": -1.5051902571561193e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 980000, "num_agent_steps_sampled": 3920000, "num_steps_trained": 980000, "num_agent_steps_trained": 3920000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 490, "training_iteration": 49, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_17-39-11", "timestamp": 1704879551, "time_this_iter_s": 470.72758746147156, "time_total_s": 24027.525684833527, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A5C10>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 24027.525684833527, "timesteps_since_restore": 0, "iterations_since_restore": 49, "perf": {"cpu_util_percent": 18.371814092953525, "ram_util_percent": 79.29265367316341}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2212.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -553.0, "policy_1": -553.0, "policy_2": -553.0, "policy_3": -553.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_1_reward": [-600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_2_reward": [-600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_3_reward": [-600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6545980787871782, "mean_inference_ms": 5.1468307837560054, "mean_action_processing_ms": 0.1290548890515961, "mean_env_wait_ms": 9.666016883101529, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1000000, "timesteps_this_iter": 0, "agent_timesteps_total": 4000000, "timers": {"sample_time_ms": 511434.502, "sample_throughput": 39.106, "load_time_ms": 1313.038, "load_throughput": 15231.85, "learn_time_ms": 190049.645, "learn_throughput": 105.236, "update_time_ms": 11.88}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1159.065185546875, "cur_kl_coeff": 3.552713678800501e-16, "cur_lr": 0.00025500000489999997, "total_loss": 2399.487744140625, "policy_loss": 2.2573471107900646e-07, "vf_loss": 2399.487744140625, "vf_explained_var": -7.510185241699219e-06, "kl": 2.0863186456221426e-10, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1441.530029296875, "cur_kl_coeff": 3.552713678800501e-16, "cur_lr": 0.00025500000489999997, "total_loss": 2391.00107421875, "policy_loss": 1.3630866906311213e-07, "vf_loss": 2391.00107421875, "vf_explained_var": -1.1920928955078125e-06, "kl": 2.899753298590824e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1150.0374755859375, "cur_kl_coeff": 3.552713678800501e-16, "cur_lr": 0.00025500000489999997, "total_loss": 2407.73173828125, "policy_loss": 1.666355106788586e-07, "vf_loss": 2407.73173828125, "vf_explained_var": 4.1961669921875e-05, "kl": -2.257595002608248e-09, "entropy": 3.218869161605835, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 966.5374755859375, "cur_kl_coeff": 3.552713678800501e-16, "cur_lr": 0.00025500000489999997, "total_loss": 2399.19658203125, "policy_loss": 2.7405739102537294e-07, "vf_loss": 2399.19658203125, "vf_explained_var": -2.086162567138672e-05, "kl": 1.1574487812637813e-09, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1000000, "num_agent_steps_sampled": 4000000, "num_steps_trained": 1000000, "num_agent_steps_trained": 4000000, "num_steps_trained_this_iter": 0}, "evaluation": {"episode_reward_max": -2000.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2240.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -500.0, "policy_1": -500.0, "policy_2": -500.0, "policy_3": -500.0}, "policy_reward_mean": {"policy_0": -560.0, "policy_1": -560.0, "policy_2": -560.0, "policy_3": -560.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0], "policy_policy_1_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0], "policy_policy_2_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0], "policy_policy_3_reward": [-500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5966905748645673, "mean_inference_ms": 5.025642882894917, "mean_action_processing_ms": 0.1267428207645404, "mean_env_wait_ms": 9.042482174406599, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}}, "done": false, "episodes_total": 500, "training_iteration": 50, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_17-51-54", "timestamp": 1704880314, "time_this_iter_s": 762.5191440582275, "time_total_s": 24790.044828891754, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001711137DEE0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 24790.044828891754, "timesteps_since_restore": 0, "iterations_since_restore": 50, "perf": {"cpu_util_percent": 17.54949026876738, "ram_util_percent": 76.60426320667284}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2204.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -551.0, "policy_1": -551.0, "policy_2": -551.0, "policy_3": -551.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2000.0, -2000.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_1_reward": [-600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_2_reward": [-600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_3_reward": [-600.0, -500.0, -500.0, -600.0, -400.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.655084098656555, "mean_inference_ms": 5.151857644841132, "mean_action_processing_ms": 0.12914924817099446, "mean_env_wait_ms": 9.666964902125178, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1020000, "timesteps_this_iter": 0, "agent_timesteps_total": 4080000, "timers": {"sample_time_ms": 539721.861, "sample_throughput": 37.056, "load_time_ms": 1312.517, "load_throughput": 15237.893, "learn_time_ms": 189922.528, "learn_throughput": 105.306, "update_time_ms": 11.88}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1024.385986328125, "cur_kl_coeff": 1.7763568394002506e-16, "cur_lr": 0.000250000005, "total_loss": 1974.763818359375, "policy_loss": 3.2506942710952557e-07, "vf_loss": 1974.763818359375, "vf_explained_var": -8.106231689453125e-06, "kl": -1.3492145810312106e-09, "entropy": 3.21886887550354, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1178.3948974609375, "cur_kl_coeff": 1.7763568394002506e-16, "cur_lr": 0.000250000005, "total_loss": 1955.598388671875, "policy_loss": 2.686595899525912e-07, "vf_loss": 1955.598388671875, "vf_explained_var": 2.8967857360839844e-05, "kl": -5.640885897273407e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1262.398681640625, "cur_kl_coeff": 1.7763568394002506e-16, "cur_lr": 0.000250000005, "total_loss": 1974.7161376953125, "policy_loss": 2.7482033253534156e-07, "vf_loss": 1974.7161376953125, "vf_explained_var": 3.248453140258789e-05, "kl": 5.035478256587566e-10, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1513.0965576171875, "cur_kl_coeff": 1.7763568394002506e-16, "cur_lr": 0.000250000005, "total_loss": 1976.83330078125, "policy_loss": 2.586841590712652e-07, "vf_loss": 1976.83330078125, "vf_explained_var": -2.7179718017578125e-05, "kl": 3.055370449700501e-10, "entropy": 3.218871593475342, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1020000, "num_agent_steps_sampled": 4080000, "num_steps_trained": 1020000, "num_agent_steps_trained": 4080000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 510, "training_iteration": 51, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_17-59-42", "timestamp": 1704880782, "time_this_iter_s": 467.9253067970276, "time_total_s": 25257.97013568878, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A59D0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 25257.97013568878, "timesteps_since_restore": 0, "iterations_since_restore": 51, "perf": {"cpu_util_percent": 17.51206636500754, "ram_util_percent": 78.82277526395174}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2200.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -550.0, "policy_1": -550.0, "policy_2": -550.0, "policy_3": -550.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -1600.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0], "policy_policy_1_reward": [-500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0], "policy_policy_2_reward": [-500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0], "policy_policy_3_reward": [-500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6555167026071426, "mean_inference_ms": 5.156540785374869, "mean_action_processing_ms": 0.1292439530357731, "mean_env_wait_ms": 9.667784510410284, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1040000, "timesteps_this_iter": 0, "agent_timesteps_total": 4160000, "timers": {"sample_time_ms": 538991.98, "sample_throughput": 37.106, "load_time_ms": 1313.016, "load_throughput": 15232.11, "learn_time_ms": 189798.213, "learn_throughput": 105.375, "update_time_ms": 11.797}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1081.7364501953125, "cur_kl_coeff": 8.881784197001253e-17, "cur_lr": 0.0002450000051, "total_loss": 2223.164111328125, "policy_loss": 2.59304046279496e-07, "vf_loss": 2223.164111328125, "vf_explained_var": -3.814697265625e-06, "kl": 2.365877388443849e-09, "entropy": 3.21887001991272, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1017.5888671875, "cur_kl_coeff": 8.881784197001253e-17, "cur_lr": 0.0002450000051, "total_loss": 2221.207177734375, "policy_loss": 3.119373323201557e-07, "vf_loss": 2221.207177734375, "vf_explained_var": 2.0265579223632812e-06, "kl": 1.580745867535427e-10, "entropy": 3.218870162963867, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1017.7029418945312, "cur_kl_coeff": 8.881784197001253e-17, "cur_lr": 0.0002450000051, "total_loss": 2225.087109375, "policy_loss": 2.7606009904701524e-07, "vf_loss": 2225.087109375, "vf_explained_var": 2.086162567138672e-05, "kl": 9.217776125725052e-10, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1269.930419921875, "cur_kl_coeff": 8.881784197001253e-17, "cur_lr": 0.0002450000051, "total_loss": 2247.235595703125, "policy_loss": 2.1811484853806463e-07, "vf_loss": 2247.235595703125, "vf_explained_var": -3.6954879760742188e-06, "kl": -2.288543313189173e-11, "entropy": 3.2188717842102053, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1040000, "num_agent_steps_sampled": 4160000, "num_steps_trained": 1040000, "num_agent_steps_trained": 4160000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 520, "training_iteration": 52, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_18-07-29", "timestamp": 1704881249, "time_this_iter_s": 467.3615097999573, "time_total_s": 25725.33164548874, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A5B80>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 25725.33164548874, "timesteps_since_restore": 0, "iterations_since_restore": 52, "perf": {"cpu_util_percent": 17.528139183055977, "ram_util_percent": 80.61966717095311}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2200.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -550.0, "policy_1": -550.0, "policy_2": -550.0, "policy_3": -550.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -1600.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0], "policy_policy_1_reward": [-600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0], "policy_policy_2_reward": [-600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0], "policy_policy_3_reward": [-600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6558320375073631, "mean_inference_ms": 5.160050710956776, "mean_action_processing_ms": 0.12931409646555767, "mean_env_wait_ms": 9.667386282868394, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1060000, "timesteps_this_iter": 0, "agent_timesteps_total": 4240000, "timers": {"sample_time_ms": 536498.547, "sample_throughput": 37.279, "load_time_ms": 1306.441, "load_throughput": 15308.77, "learn_time_ms": 189398.769, "learn_throughput": 105.597, "update_time_ms": 11.597}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1630.8133544921875, "cur_kl_coeff": 4.4408920985006264e-17, "cur_lr": 0.00024000000519999997, "total_loss": 2183.612255859375, "policy_loss": 2.5424003524676664e-07, "vf_loss": 2183.612255859375, "vf_explained_var": 1.233816146850586e-05, "kl": 2.901908609242998e-11, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1436.640625, "cur_kl_coeff": 4.4408920985006264e-17, "cur_lr": 0.00024000000519999997, "total_loss": 2165.8171875, "policy_loss": 4.280090451835861e-07, "vf_loss": 2165.8171875, "vf_explained_var": 9.059906005859375e-06, "kl": -9.262509093055371e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1452.599365234375, "cur_kl_coeff": 4.4408920985006264e-17, "cur_lr": 0.00024000000519999997, "total_loss": 2186.25146484375, "policy_loss": 2.0440101491381313e-07, "vf_loss": 2186.25146484375, "vf_explained_var": 3.55839729309082e-05, "kl": 6.634622948253899e-10, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1385.89453125, "cur_kl_coeff": 4.4408920985006264e-17, "cur_lr": 0.00024000000519999997, "total_loss": 2176.04306640625, "policy_loss": 2.480316173114616e-07, "vf_loss": 2176.04306640625, "vf_explained_var": -1.2993812561035156e-05, "kl": 9.354174324016639e-10, "entropy": 3.218871593475342, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1060000, "num_agent_steps_sampled": 4240000, "num_steps_trained": 1060000, "num_agent_steps_trained": 4240000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 530, "training_iteration": 53, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_18-15-15", "timestamp": 1704881715, "time_this_iter_s": 465.7571711540222, "time_total_s": 26191.08881664276, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A55E0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 26191.08881664276, "timesteps_since_restore": 0, "iterations_since_restore": 53, "perf": {"cpu_util_percent": 17.543636363636363, "ram_util_percent": 81.69030303030304}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2204.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -551.0, "policy_1": -551.0, "policy_2": -551.0, "policy_3": -551.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -1600.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0], "policy_policy_1_reward": [-600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0], "policy_policy_2_reward": [-600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0], "policy_policy_3_reward": [-600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6560074131406808, "mean_inference_ms": 5.162357283472478, "mean_action_processing_ms": 0.12936017214507012, "mean_env_wait_ms": 9.66584094802039, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1080000, "timesteps_this_iter": 0, "agent_timesteps_total": 4320000, "timers": {"sample_time_ms": 533679.257, "sample_throughput": 37.476, "load_time_ms": 1298.773, "load_throughput": 15399.153, "learn_time_ms": 188638.793, "learn_throughput": 106.023, "update_time_ms": 11.705}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1159.6302490234375, "cur_kl_coeff": 2.2204460492503132e-17, "cur_lr": 0.0002350000053, "total_loss": 2214.244873046875, "policy_loss": 1.723289548394291e-07, "vf_loss": 2214.244873046875, "vf_explained_var": -7.3909759521484375e-06, "kl": 9.93249066327806e-10, "entropy": 3.218869113922119, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1121.15087890625, "cur_kl_coeff": 2.2204460492503132e-17, "cur_lr": 0.0002350000053, "total_loss": 2215.86845703125, "policy_loss": 1.624012018516807e-07, "vf_loss": 2215.86845703125, "vf_explained_var": 1.138448715209961e-05, "kl": 3.931234754395163e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1208.581787109375, "cur_kl_coeff": 2.2204460492503132e-17, "cur_lr": 0.0002350000053, "total_loss": 2240.7755859375, "policy_loss": 2.064132702095378e-07, "vf_loss": 2240.7755859375, "vf_explained_var": 2.193450927734375e-05, "kl": 5.587207130980377e-10, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1291.083251953125, "cur_kl_coeff": 2.2204460492503132e-17, "cur_lr": 0.0002350000053, "total_loss": 2236.781689453125, "policy_loss": 3.390979822270879e-07, "vf_loss": 2236.781689453125, "vf_explained_var": -1.0967254638671875e-05, "kl": 1.4147779295314856e-09, "entropy": 3.2188717842102053, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1080000, "num_agent_steps_sampled": 4320000, "num_steps_trained": 1080000, "num_agent_steps_trained": 4320000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 540, "training_iteration": 54, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_18-23-03", "timestamp": 1704882183, "time_this_iter_s": 467.2229950428009, "time_total_s": 26658.311811685562, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A5A60>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 26658.311811685562, "timesteps_since_restore": 0, "iterations_since_restore": 54, "perf": {"cpu_util_percent": 17.521936459909227, "ram_util_percent": 84.47413010590014}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2188.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -547.0, "policy_1": -547.0, "policy_2": -547.0, "policy_3": -547.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -1600.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0], "policy_policy_1_reward": [-500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0], "policy_policy_2_reward": [-500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0], "policy_policy_3_reward": [-500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.656096975498906, "mean_inference_ms": 5.163931181498375, "mean_action_processing_ms": 0.12938053807616445, "mean_env_wait_ms": 9.663598055093853, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1100000, "timesteps_this_iter": 0, "agent_timesteps_total": 4400000, "timers": {"sample_time_ms": 531312.92, "sample_throughput": 37.643, "load_time_ms": 1291.097, "load_throughput": 15490.7, "learn_time_ms": 188141.761, "learn_throughput": 106.303, "update_time_ms": 12.104}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1107.1131591796875, "cur_kl_coeff": 1.1102230246251566e-17, "cur_lr": 0.00023000000539999998, "total_loss": 2074.51328125, "policy_loss": 1.0882377607934756e-07, "vf_loss": 2074.51328125, "vf_explained_var": 1.811981201171875e-05, "kl": 1.2847664293547112e-09, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1119.6021728515625, "cur_kl_coeff": 1.1102230246251566e-17, "cur_lr": 0.00023000000539999998, "total_loss": 2075.57705078125, "policy_loss": 2.5073051204138606e-07, "vf_loss": 2075.57705078125, "vf_explained_var": 7.212162017822266e-06, "kl": 1.233401960426539e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1001.8772583007812, "cur_kl_coeff": 1.1102230246251566e-17, "cur_lr": 0.00023000000539999998, "total_loss": 2085.32021484375, "policy_loss": 3.589630085620854e-07, "vf_loss": 2085.32021484375, "vf_explained_var": 3.3795833587646484e-05, "kl": 1.6700212296383298e-09, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1131.1981201171875, "cur_kl_coeff": 1.1102230246251566e-17, "cur_lr": 0.00023000000539999998, "total_loss": 2089.708642578125, "policy_loss": 2.236080174800037e-07, "vf_loss": 2089.708642578125, "vf_explained_var": -1.1682510375976562e-05, "kl": -8.799476730603218e-10, "entropy": 3.218871736526489, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1100000, "num_agent_steps_sampled": 4400000, "num_steps_trained": 1100000, "num_agent_steps_trained": 4400000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 550, "training_iteration": 55, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_18-30-51", "timestamp": 1704882651, "time_this_iter_s": 468.45096611976624, "time_total_s": 27126.76277780533, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x0000017102DA3700>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 27126.76277780533, "timesteps_since_restore": 0, "iterations_since_restore": 55, "perf": {"cpu_util_percent": 17.666867469879517, "ram_util_percent": 85.61340361445784}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2200.0, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -550.0, "policy_1": -550.0, "policy_2": -550.0, "policy_3": -550.0}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -1600.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0], "policy_policy_1_reward": [-500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0], "policy_policy_2_reward": [-500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0], "policy_policy_3_reward": [-500.0, -600.0, -400.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6561781144009194, "mean_inference_ms": 5.165429618198379, "mean_action_processing_ms": 0.1294071896372443, "mean_env_wait_ms": 9.66131524021397, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1120000, "timesteps_this_iter": 0, "agent_timesteps_total": 4480000, "timers": {"sample_time_ms": 530419.447, "sample_throughput": 37.706, "load_time_ms": 1293.286, "load_throughput": 15464.483, "learn_time_ms": 188118.835, "learn_throughput": 106.316, "update_time_ms": 12.215}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1329.85791015625, "cur_kl_coeff": 5.551115123125783e-18, "cur_lr": 0.00022500000549999997, "total_loss": 2459.5681640625, "policy_loss": 2.840137455883962e-07, "vf_loss": 2459.5681640625, "vf_explained_var": 1.7881393432617188e-06, "kl": 1.4701933503946662e-09, "entropy": 3.218869352340698, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1636.725830078125, "cur_kl_coeff": 5.551115123125783e-18, "cur_lr": 0.00022500000549999997, "total_loss": 2437.59345703125, "policy_loss": 3.760337770053468e-07, "vf_loss": 2437.59345703125, "vf_explained_var": -5.960464477539062e-07, "kl": 1.4072277232379804e-09, "entropy": 3.218870162963867, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1148.1715087890625, "cur_kl_coeff": 5.551115123125783e-18, "cur_lr": 0.00022500000549999997, "total_loss": 2456.229052734375, "policy_loss": 2.1270751489055327e-07, "vf_loss": 2456.229052734375, "vf_explained_var": 3.230571746826172e-05, "kl": 1.6496392231390989e-09, "entropy": 3.218868780136108, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1402.5648193359375, "cur_kl_coeff": 5.551115123125783e-18, "cur_lr": 0.00022500000549999997, "total_loss": 2450.287109375, "policy_loss": 2.661609689980082e-07, "vf_loss": 2450.287109375, "vf_explained_var": -2.3484230041503906e-05, "kl": -7.53298259881241e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1120000, "num_agent_steps_sampled": 4480000, "num_steps_trained": 1120000, "num_agent_steps_trained": 4480000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 560, "training_iteration": 56, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_18-38-38", "timestamp": 1704883118, "time_this_iter_s": 466.5814678668976, "time_total_s": 27593.344245672226, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170441E0DC0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 27593.344245672226, "timesteps_since_restore": 0, "iterations_since_restore": 56, "perf": {"cpu_util_percent": 17.675303030303027, "ram_util_percent": 88.27863636363635}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2199.64, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -549.91, "policy_1": -549.91, "policy_2": -549.91, "policy_3": -549.91}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -1600.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2364.0, -2400.0, -2400.0, -1600.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0], "policy_policy_1_reward": [-600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0], "policy_policy_2_reward": [-600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0], "policy_policy_3_reward": [-600.0, -600.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6560193354253037, "mean_inference_ms": 5.164603930651526, "mean_action_processing_ms": 0.1293814124617834, "mean_env_wait_ms": 9.656248949095875, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1140000, "timesteps_this_iter": 0, "agent_timesteps_total": 4560000, "timers": {"sample_time_ms": 525046.042, "sample_throughput": 38.092, "load_time_ms": 1265.07, "load_throughput": 15809.396, "learn_time_ms": 171147.336, "learn_throughput": 116.858, "update_time_ms": 12.238}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1406.7349853515625, "cur_kl_coeff": 2.7755575615628915e-18, "cur_lr": 0.00022000000559999994, "total_loss": 2259.930126953125, "policy_loss": 3.544807490207802e-07, "vf_loss": 2259.930126953125, "vf_explained_var": 8.940696716308594e-06, "kl": 1.3813104271509502e-09, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1482.7181396484375, "cur_kl_coeff": 2.7755575615628915e-18, "cur_lr": 0.00022000000559999994, "total_loss": 2268.84287109375, "policy_loss": 2.8936385412237085e-07, "vf_loss": 2268.84287109375, "vf_explained_var": 3.933906555175781e-06, "kl": 7.407950947113662e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 2157.175048828125, "cur_kl_coeff": 2.7755575615628915e-18, "cur_lr": 0.00022000000559999994, "total_loss": 2277.183447265625, "policy_loss": 4.013061532637252e-07, "vf_loss": 2277.183447265625, "vf_explained_var": 1.9788742065429688e-05, "kl": 1.5088616637810166e-09, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1752.0908203125, "cur_kl_coeff": 2.7755575615628915e-18, "cur_lr": 0.00022000000559999994, "total_loss": 2261.5533203125, "policy_loss": 3.541469569268152e-07, "vf_loss": 2261.5533203125, "vf_explained_var": 1.0192394256591797e-05, "kl": -2.022215839225794e-10, "entropy": 3.218871593475342, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1140000, "num_agent_steps_sampled": 4560000, "num_steps_trained": 1140000, "num_agent_steps_trained": 4560000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 570, "training_iteration": 57, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_18-46-25", "timestamp": 1704883585, "time_this_iter_s": 467.1145520210266, "time_total_s": 28060.458797693253, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A5790>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 28060.458797693253, "timesteps_since_restore": 0, "iterations_since_restore": 57, "perf": {"cpu_util_percent": 17.436404833836857, "ram_util_percent": 88.68187311178248}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2207.64, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -551.91, "policy_1": -551.91, "policy_2": -551.91, "policy_3": -551.91}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -1600.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2364.0, -2400.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2000.0, -1600.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_1_reward": [-500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_2_reward": [-500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0], "policy_policy_3_reward": [-500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6554747031358897, "mean_inference_ms": 5.160586189540023, "mean_action_processing_ms": 0.12930559002812042, "mean_env_wait_ms": 9.64745773911068, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1160000, "timesteps_this_iter": 0, "agent_timesteps_total": 4640000, "timers": {"sample_time_ms": 500618.13, "sample_throughput": 39.951, "load_time_ms": 1233.0, "load_throughput": 16220.594, "learn_time_ms": 167783.343, "learn_throughput": 119.201, "update_time_ms": 12.241}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1146.34130859375, "cur_kl_coeff": 1.3877787807814458e-18, "cur_lr": 0.0002150000057, "total_loss": 2083.373486328125, "policy_loss": 2.672004700735031e-07, "vf_loss": 2083.373486328125, "vf_explained_var": 2.008676528930664e-05, "kl": -4.791651166125011e-10, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1059.091796875, "cur_kl_coeff": 1.3877787807814458e-18, "cur_lr": 0.0002150000057, "total_loss": 2064.060986328125, "policy_loss": 4.943466211715375e-07, "vf_loss": 2064.060986328125, "vf_explained_var": 2.092123031616211e-05, "kl": 4.56735094189753e-11, "entropy": 3.2188694000244142, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1124.443603515625, "cur_kl_coeff": 1.3877787807814458e-18, "cur_lr": 0.0002150000057, "total_loss": 2066.60888671875, "policy_loss": 2.2546768780884462e-07, "vf_loss": 2066.60888671875, "vf_explained_var": 4.3392181396484375e-05, "kl": -3.749997867785737e-10, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1034.7076416015625, "cur_kl_coeff": 1.3877787807814458e-18, "cur_lr": 0.0002150000057, "total_loss": 2080.73203125, "policy_loss": 2.3890495617351346e-07, "vf_loss": 2080.73203125, "vf_explained_var": -2.6226043701171875e-05, "kl": -1.3431532275287594e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1160000, "num_agent_steps_sampled": 4640000, "num_steps_trained": 1160000, "num_agent_steps_trained": 4640000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 580, "training_iteration": 58, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_18-54-13", "timestamp": 1704884053, "time_this_iter_s": 468.04259276390076, "time_total_s": 28528.501390457153, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x0000017102DA3940>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 28528.501390457153, "timesteps_since_restore": 0, "iterations_since_restore": 58, "perf": {"cpu_util_percent": 17.727149321266968, "ram_util_percent": 92.1499245852187}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2203.64, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -550.91, "policy_1": -550.91, "policy_2": -550.91, "policy_3": -550.91}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2400.0, -2400.0, -1600.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -1600.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2364.0, -2400.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2000.0, -1600.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0], "policy_policy_1_reward": [-600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0], "policy_policy_2_reward": [-600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0], "policy_policy_3_reward": [-600.0, -600.0, -600.0, -400.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6549142705382687, "mean_inference_ms": 5.15657762756659, "mean_action_processing_ms": 0.12922436455007374, "mean_env_wait_ms": 9.638685182508272, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1180000, "timesteps_this_iter": 0, "agent_timesteps_total": 4720000, "timers": {"sample_time_ms": 496717.265, "sample_throughput": 40.264, "load_time_ms": 1234.628, "load_throughput": 16199.21, "learn_time_ms": 167803.545, "learn_throughput": 119.187, "update_time_ms": 12.022}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1078.93994140625, "cur_kl_coeff": 6.938893903907229e-19, "cur_lr": 0.00021000000579999997, "total_loss": 2150.5212890625, "policy_loss": 2.36291885080675e-07, "vf_loss": 2150.5212890625, "vf_explained_var": 7.748603820800781e-07, "kl": 9.41158678768872e-11, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1041.037841796875, "cur_kl_coeff": 6.938893903907229e-19, "cur_lr": 0.00021000000579999997, "total_loss": 2149.932861328125, "policy_loss": 3.027915931319569e-07, "vf_loss": 2149.932861328125, "vf_explained_var": 3.725290298461914e-05, "kl": 8.307519561157051e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1120.293212890625, "cur_kl_coeff": 6.938893903907229e-19, "cur_lr": 0.00021000000579999997, "total_loss": 2141.2966796875, "policy_loss": 2.6832580894975423e-07, "vf_loss": 2141.2966796875, "vf_explained_var": 1.8417835235595703e-05, "kl": 2.8425786890995395e-09, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1183.412841796875, "cur_kl_coeff": 6.938893903907229e-19, "cur_lr": 0.00021000000579999997, "total_loss": 2149.357275390625, "policy_loss": 4.1702270621168934e-07, "vf_loss": 2149.357275390625, "vf_explained_var": -2.9802322387695312e-05, "kl": -6.310924588470713e-10, "entropy": 3.218871736526489, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1180000, "num_agent_steps_sampled": 4720000, "num_steps_trained": 1180000, "num_agent_steps_trained": 4720000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 590, "training_iteration": 59, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_19-01-59", "timestamp": 1704884519, "time_this_iter_s": 465.8682265281677, "time_total_s": 28994.36961698532, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x000001710A8A5700>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 28994.36961698532, "timesteps_since_restore": 0, "iterations_since_restore": 59, "perf": {"cpu_util_percent": 17.571969696969695, "ram_util_percent": 90.93272727272728}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2207.64, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -551.91, "policy_1": -551.91, "policy_2": -551.91, "policy_3": -551.91}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -1600.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2364.0, -2400.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2000.0, -1600.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_1_reward": [-600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_2_reward": [-600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0], "policy_policy_3_reward": [-600.0, -500.0, -500.0, -500.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6543958310927934, "mean_inference_ms": 5.152700774467238, "mean_action_processing_ms": 0.12914701231178097, "mean_env_wait_ms": 9.630227418646642, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1200000, "timesteps_this_iter": 0, "agent_timesteps_total": 4800000, "timers": {"sample_time_ms": 496750.739, "sample_throughput": 40.262, "load_time_ms": 1237.694, "load_throughput": 16159.088, "learn_time_ms": 167951.34, "learn_throughput": 119.082, "update_time_ms": 12.218}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1262.2674560546875, "cur_kl_coeff": 3.4694469519536144e-19, "cur_lr": 0.0002050000059, "total_loss": 2313.425927734375, "policy_loss": 3.588295007794784e-07, "vf_loss": 2313.425927734375, "vf_explained_var": 1.4007091522216797e-05, "kl": -6.60857168988116e-10, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1115.490966796875, "cur_kl_coeff": 3.4694469519536144e-19, "cur_lr": 0.0002050000059, "total_loss": 2300.28271484375, "policy_loss": 6.287479359379233e-07, "vf_loss": 2300.28271484375, "vf_explained_var": 1.537799835205078e-05, "kl": 1.488822787276689e-09, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1378.14013671875, "cur_kl_coeff": 3.4694469519536144e-19, "cur_lr": 0.0002050000059, "total_loss": 2311.099658203125, "policy_loss": 3.153324055649165e-07, "vf_loss": 2311.099658203125, "vf_explained_var": 2.288818359375e-05, "kl": -2.338962176717807e-10, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1278.0936279296875, "cur_kl_coeff": 3.4694469519536144e-19, "cur_lr": 0.0002050000059, "total_loss": 2316.48994140625, "policy_loss": 2.33068468791231e-07, "vf_loss": 2316.48994140625, "vf_explained_var": -1.5735626220703125e-05, "kl": -5.467470670650343e-10, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1200000, "num_agent_steps_sampled": 4800000, "num_steps_trained": 1200000, "num_agent_steps_trained": 4800000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 600, "training_iteration": 60, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_19-09-47", "timestamp": 1704884987, "time_this_iter_s": 467.91772294044495, "time_total_s": 29462.287339925766, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x00000170443ACF70>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 29462.287339925766, "timesteps_since_restore": 0, "iterations_since_restore": 60, "perf": {"cpu_util_percent": 17.723716012084594, "ram_util_percent": 92.80528700906345}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2219.64, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -554.91, "policy_1": -554.91, "policy_2": -554.91, "policy_3": -554.91}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2400.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -1600.0, -2000.0, -2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2364.0, -2400.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2000.0, -1600.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0], "policy_policy_1_reward": [-600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0], "policy_policy_2_reward": [-600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0], "policy_policy_3_reward": [-600.0, -500.0, -500.0, -600.0, -500.0, -600.0, -500.0, -600.0, -400.0, -500.0, -500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6539118920774578, "mean_inference_ms": 5.148960791621372, "mean_action_processing_ms": 0.1290740083834232, "mean_env_wait_ms": 9.621871965731469, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1220000, "timesteps_this_iter": 0, "agent_timesteps_total": 4880000, "timers": {"sample_time_ms": 467067.375, "sample_throughput": 42.82, "load_time_ms": 1248.669, "load_throughput": 16017.052, "learn_time_ms": 167957.613, "learn_throughput": 119.078, "update_time_ms": 12.228}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1160.9351806640625, "cur_kl_coeff": 1.7347234759768072e-19, "cur_lr": 0.000200000006, "total_loss": 2224.60615234375, "policy_loss": 2.396392783321488e-07, "vf_loss": 2224.60615234375, "vf_explained_var": -1.1086463928222656e-05, "kl": 2.13163809936745e-09, "entropy": 3.2188690185546873, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1245.498291015625, "cur_kl_coeff": 1.7347234759768072e-19, "cur_lr": 0.000200000006, "total_loss": 2223.7177734375, "policy_loss": 2.2393226632999587e-07, "vf_loss": 2223.7177734375, "vf_explained_var": -5.4836273193359375e-06, "kl": 1.8469158868938962e-09, "entropy": 3.218870258331299, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1581.749267578125, "cur_kl_coeff": 1.7347234759768072e-19, "cur_lr": 0.000200000006, "total_loss": 2241.726708984375, "policy_loss": 1.249408709325106e-07, "vf_loss": 2241.726708984375, "vf_explained_var": 1.4960765838623047e-05, "kl": -1.4049290231188394e-09, "entropy": 3.2188687324523926, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1610.886962890625, "cur_kl_coeff": 1.7347234759768072e-19, "cur_lr": 0.000200000006, "total_loss": 2235.537109375, "policy_loss": 2.2604941665171908e-07, "vf_loss": 2235.537109375, "vf_explained_var": -1.0251998901367188e-05, "kl": 3.2219159631918794e-12, "entropy": 3.218871831893921, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1220000, "num_agent_steps_sampled": 4880000, "num_steps_trained": 1220000, "num_agent_steps_trained": 4880000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 610, "training_iteration": 61, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_19-17-33", "timestamp": 1704885453, "time_this_iter_s": 466.0320370197296, "time_total_s": 29928.319376945496, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x0000017102DA3550>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 29928.319376945496, "timesteps_since_restore": 0, "iterations_since_restore": 61, "perf": {"cpu_util_percent": 17.629393939393943, "ram_util_percent": 93.59000000000002}}
{"episode_reward_max": -1600.0, "episode_reward_min": -2400.0, "episode_reward_mean": -2223.64, "episode_len_mean": 2000.0, "episode_media": {}, "episodes_this_iter": 10, "policy_reward_min": {"policy_0": -600.0, "policy_1": -600.0, "policy_2": -600.0, "policy_3": -600.0}, "policy_reward_max": {"policy_0": -400.0, "policy_1": -400.0, "policy_2": -400.0, "policy_3": -400.0}, "policy_reward_mean": {"policy_0": -555.91, "policy_1": -555.91, "policy_2": -555.91, "policy_3": -555.91}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-2000.0, -2000.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2000.0, -1600.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2364.0, -2400.0, -2400.0, -1600.0, -2400.0, -2400.0, -2400.0, -2000.0, -1600.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2400.0, -2000.0, -2000.0, -2000.0, -2400.0, -2400.0, -2400.0, -2000.0, -2000.0, -2400.0, -2000.0], "episode_lengths": [2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000], "policy_policy_0_reward": [-500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0], "policy_policy_1_reward": [-500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0], "policy_policy_2_reward": [-500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0], "policy_policy_3_reward": [-500.0, -500.0, -500.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -500.0, -400.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -591.0, -600.0, -600.0, -400.0, -600.0, -600.0, -600.0, -500.0, -400.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -600.0, -500.0, -500.0, -500.0, -600.0, -600.0, -600.0, -500.0, -500.0, -600.0, -500.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6534591358873455, "mean_inference_ms": 5.1453724351408265, "mean_action_processing_ms": 0.1289972228831309, "mean_env_wait_ms": 9.613753144962248, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1240000, "timesteps_this_iter": 0, "agent_timesteps_total": 4960000, "timers": {"sample_time_ms": 467058.231, "sample_throughput": 42.821, "load_time_ms": 1248.581, "load_throughput": 16018.189, "learn_time_ms": 175505.726, "learn_throughput": 113.956, "update_time_ms": 14.75}, "info": {"learner": {"policy_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1202.1942138671875, "cur_kl_coeff": 8.673617379884036e-20, "cur_lr": 0.00019500000609999998, "total_loss": 2168.535302734375, "policy_loss": 3.037929444005272e-07, "vf_loss": 2168.535302734375, "vf_explained_var": 6.735324859619141e-06, "kl": 6.707021594359874e-10, "entropy": 3.218869209289551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1091.3968505859375, "cur_kl_coeff": 8.673617379884036e-20, "cur_lr": 0.00019500000609999998, "total_loss": 2157.74638671875, "policy_loss": 2.1185875143103772e-07, "vf_loss": 2157.74638671875, "vf_explained_var": 9.775161743164062e-06, "kl": -6.465976531977447e-10, "entropy": 3.2188704013824463, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1067.510009765625, "cur_kl_coeff": 8.673617379884036e-20, "cur_lr": 0.00019500000609999998, "total_loss": 2165.630810546875, "policy_loss": 1.9432067495106508e-07, "vf_loss": 2165.630810546875, "vf_explained_var": 2.5570392608642578e-05, "kl": 2.008678348652815e-09, "entropy": 3.2188689708709717, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 1118.123779296875, "cur_kl_coeff": 8.673617379884036e-20, "cur_lr": 0.00019500000609999998, "total_loss": 2169.587060546875, "policy_loss": 3.385829995039558e-07, "vf_loss": 2169.587060546875, "vf_explained_var": -2.9921531677246094e-05, "kl": 3.12598402718578e-10, "entropy": 3.218871593475342, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1240000, "num_agent_steps_sampled": 4960000, "num_steps_trained": 1240000, "num_agent_steps_trained": 4960000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 620, "training_iteration": 62, "trial_id": "2714e_00000", "experiment_id": "f54f3430f83f4024be3da2226c700319", "date": "2024-01-10_19-26-36", "timestamp": 1704885996, "time_this_iter_s": 542.5798614025116, "time_total_s": 30470.899238348007, "pid": 13456, "hostname": "DESKTOP-CC49RT1", "node_ip": "127.0.0.1", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 200, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 20000, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": true, "use_lstm": false, "max_seq_len": 2000, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mate", "env_args": {"continuous_actions_camera": false, "continuous_actions_target": false, "discrete_levels": 5, "coop_team": "target", "map_name": "MATE-4v4-9-v0"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": true, "local_mode": true, "share_policy": "individual", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 100, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "gru", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "happo", "actor_lr": 5e-07, "critic_lr": 0.0005, "gain": 0.01, "space_obs": "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "space_act": "Discrete(25)", "num_agents": 4, "episode_limit": 2000, "policy_mapping_info": {"all_scenario": {"description": "mate single team multi-agent scenarios", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["agent_0", "agent_1", "agent_2", "agent_3"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": 2000, "soft_horizon": false, "no_done_at_end": false, "env": "mate_MATE-4v4-9-v0", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_0": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_1": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_2": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}], "policy_3": ["<class 'ray.rllib.policy.policy_template.HAPPOTorchPolicy'>", "Dict(obs:Box([ 0.e+00  0.e+00  0.e+00  0.e+00 -2.e+03 -2.e+03 -2.e+03 -2.e+03 -2.e+03\n -2.e+03 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00  0.e+00\n  1.e+00  0.e+00  0.e+00  0.e+00  0.e+00 -1.e+00 -1.e+00 -1.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00\n -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03\n -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00\n -1.e+00 -1.e+00 -2.e+03 -2.e+03  0.e+00 -1.e+00 -1.e+00 -2.e+03 -2.e+03\n  0.e+00 -1.e+00 -1.e+00], [    inf     inf     inf     inf 2.0e+03 2.0e+03 2.0e+03 2.0e+03 2.0e+03\n 2.0e+03 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 2.0e+03 1.0e+00 2.0e+03\n 2.0e+00     inf     inf     inf     inf 1.0e+00 1.0e+00 1.0e+00 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 2.0e+03 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03\n 2.0e+03 1.8e+02 1.0e+00 2.0e+03 2.0e+03 1.0e+03 2.0e+03 2.0e+03 1.8e+02\n 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00\n 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03\n 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03\n 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03 1.0e+00 2.0e+03 2.0e+03 1.0e+03\n 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03\n 1.0e+00 1.0e+00 2.0e+03 2.0e+03 2.0e+03 1.0e+00 1.0e+00 2.0e+03 2.0e+03\n 2.0e+03 1.0e+00 1.0e+00], (111,), float64))", "Discrete(25)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x0000017102DA3670>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 20000, "shuffle_sequences": true, "num_sgd_iter": 5, "lr_schedule": [[0, 0.0005], [2000000, 1e-11]], "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": 10, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 30470.899238348007, "timesteps_since_restore": 0, "iterations_since_restore": 62, "perf": {"cpu_util_percent": 17.771968709256843, "ram_util_percent": 93.63976531942635}}
